{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2021 CNRS\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in all\n",
    "# copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "import warnings\n",
    "from functools import cached_property\n",
    "from pathlib import Path\n",
    "from typing import Optional, Text, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchaudio.compliance.kaldi as kaldi\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub.utils import RepositoryNotFoundError\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from pyannote.audio import Inference, Model, Pipeline\n",
    "from pyannote.audio.core.inference import BaseInference\n",
    "from pyannote.audio.core.io import AudioFile\n",
    "from pyannote.audio.core.model import CACHE_DIR\n",
    "from pyannote.audio.pipelines.utils import PipelineModel, get_model\n",
    "\n",
    "try:\n",
    "    from speechbrain.pretrained import (\n",
    "        EncoderClassifier as SpeechBrain_EncoderClassifier,\n",
    "    )\n",
    "\n",
    "    SPEECHBRAIN_IS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SPEECHBRAIN_IS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from nemo.collections.asr.models import (\n",
    "        EncDecSpeakerLabelModel as NeMo_EncDecSpeakerLabelModel,\n",
    "    )\n",
    "\n",
    "    NEMO_IS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    NEMO_IS_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "\n",
    "    ONNX_IS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ONNX_IS_AVAILABLE = False\n",
    "\n",
    "\n",
    "class NeMoPretrainedSpeakerEmbedding(BaseInference):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding: Text = \"nvidia/speakerverification_en_titanet_large\",\n",
    "        device: Optional[torch.device] = None,\n",
    "    ):\n",
    "        if not NEMO_IS_AVAILABLE:\n",
    "            raise ImportError(\n",
    "                f\"'NeMo' must be installed to use '{embedding}' embeddings. \"\n",
    "                \"Visit https://nvidia.github.io/NeMo/ for installation instructions.\"\n",
    "            )\n",
    "\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.device = device or torch.device(\"cpu\")\n",
    "\n",
    "        self.model_ = NeMo_EncDecSpeakerLabelModel.from_pretrained(self.embedding)\n",
    "        self.model_.freeze()\n",
    "        self.model_.to(self.device)\n",
    "\n",
    "    def to(self, device: torch.device):\n",
    "        if not isinstance(device, torch.device):\n",
    "            raise TypeError(\n",
    "                f\"`device` must be an instance of `torch.device`, got `{type(device).__name__}`\"\n",
    "            )\n",
    "\n",
    "        self.model_.to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    @cached_property\n",
    "    def sample_rate(self) -> int:\n",
    "        return self.model_._cfg.train_ds.get(\"sample_rate\", 16000)\n",
    "\n",
    "    @cached_property\n",
    "    def dimension(self) -> int:\n",
    "        input_signal = torch.rand(1, self.sample_rate).to(self.device)\n",
    "        input_signal_length = torch.tensor([self.sample_rate]).to(self.device)\n",
    "        _, embeddings = self.model_(\n",
    "            input_signal=input_signal, input_signal_length=input_signal_length\n",
    "        )\n",
    "        _, dimension = embeddings.shape\n",
    "        return dimension\n",
    "\n",
    "    @cached_property\n",
    "    def metric(self) -> str:\n",
    "        return \"cosine\"\n",
    "\n",
    "    @cached_property\n",
    "    def min_num_samples(self) -> int:\n",
    "        lower, upper = 2, round(0.5 * self.sample_rate)\n",
    "        middle = (lower + upper) // 2\n",
    "        while lower + 1 < upper:\n",
    "            try:\n",
    "                input_signal = torch.rand(1, middle).to(self.device)\n",
    "                input_signal_length = torch.tensor([middle]).to(self.device)\n",
    "\n",
    "                _ = self.model_(\n",
    "                    input_signal=input_signal, input_signal_length=input_signal_length\n",
    "                )\n",
    "\n",
    "                upper = middle\n",
    "            except RuntimeError:\n",
    "                lower = middle\n",
    "\n",
    "            middle = (lower + upper) // 2\n",
    "\n",
    "        return upper\n",
    "\n",
    "    def __call__(\n",
    "        self, waveforms: torch.Tensor, masks: Optional[torch.Tensor] = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        waveforms : (batch_size, num_channels, num_samples)\n",
    "            Only num_channels == 1 is supported.\n",
    "        masks : (batch_size, num_samples), optional\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        embeddings : (batch_size, dimension)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, num_channels, num_samples = waveforms.shape\n",
    "        assert num_channels == 1\n",
    "\n",
    "        waveforms = waveforms.squeeze(dim=1)\n",
    "\n",
    "        if masks is None:\n",
    "            signals = waveforms.squeeze(dim=1)\n",
    "            wav_lens = signals.shape[1] * torch.ones(batch_size)\n",
    "\n",
    "        else:\n",
    "            batch_size_masks, _ = masks.shape\n",
    "            assert batch_size == batch_size_masks\n",
    "\n",
    "            # TODO: speed up the creation of \"signals\"\n",
    "            # preliminary profiling experiments show\n",
    "            # that it accounts for 15% of __call__\n",
    "            # (the remaining 85% being the actual forward pass)\n",
    "\n",
    "            imasks = F.interpolate(\n",
    "                masks.unsqueeze(dim=1), size=num_samples, mode=\"nearest\"\n",
    "            ).squeeze(dim=1)\n",
    "\n",
    "            imasks = imasks > 0.5\n",
    "\n",
    "            signals = pad_sequence(\n",
    "                [waveform[imask] for waveform, imask in zip(waveforms, imasks)],\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "            wav_lens = imasks.sum(dim=1)\n",
    "\n",
    "        max_len = wav_lens.max()\n",
    "\n",
    "        # corner case: every signal is too short\n",
    "        if max_len < self.min_num_samples:\n",
    "            return np.NAN * np.zeros((batch_size, self.dimension))\n",
    "\n",
    "        too_short = wav_lens < self.min_num_samples\n",
    "        wav_lens[too_short] = max_len\n",
    "\n",
    "        _, embeddings = self.model_(\n",
    "            input_signal=waveforms.to(self.device),\n",
    "            input_signal_length=wav_lens.to(self.device),\n",
    "        )\n",
    "\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "        embeddings[too_short.cpu().numpy()] = np.NAN\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class SpeechBrainPretrainedSpeakerEmbedding(BaseInference):\n",
    "    \"\"\"Pretrained SpeechBrain speaker embedding\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding : str\n",
    "        Name of SpeechBrain model\n",
    "    device : torch.device, optional\n",
    "        Device\n",
    "    use_auth_token : str, optional\n",
    "        When loading private huggingface.co models, set `use_auth_token`\n",
    "        to True or to a string containing your hugginface.co authentication\n",
    "        token that can be obtained by running `huggingface-cli login`\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    >>> get_embedding = SpeechBrainPretrainedSpeakerEmbedding(\"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "    >>> assert waveforms.ndim == 3\n",
    "    >>> batch_size, num_channels, num_samples = waveforms.shape\n",
    "    >>> assert num_channels == 1\n",
    "    >>> embeddings = get_embedding(waveforms)\n",
    "    >>> assert embeddings.ndim == 2\n",
    "    >>> assert embeddings.shape[0] == batch_size\n",
    "\n",
    "    >>> assert binary_masks.ndim == 1\n",
    "    >>> assert binary_masks.shape[0] == batch_size\n",
    "    >>> embeddings = get_embedding(waveforms, masks=binary_masks)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding: Text = \"speechbrain/spkrec-ecapa-voxceleb\",\n",
    "        device: Optional[torch.device] = None,\n",
    "        use_auth_token: Union[Text, None] = None,\n",
    "    ):\n",
    "        if not SPEECHBRAIN_IS_AVAILABLE:\n",
    "            raise ImportError(\n",
    "                f\"'speechbrain' must be installed to use '{embedding}' embeddings. \"\n",
    "                \"Visit https://speechbrain.github.io for installation instructions.\"\n",
    "            )\n",
    "\n",
    "        super().__init__()\n",
    "        if \"@\" in embedding:\n",
    "            self.embedding = embedding.split(\"@\")[0]\n",
    "            self.revision = embedding.split(\"@\")[1]\n",
    "        else:\n",
    "            self.embedding = embedding\n",
    "            self.revision = None\n",
    "        self.device = device or torch.device(\"cpu\")\n",
    "        self.use_auth_token = use_auth_token\n",
    "\n",
    "        self.classifier_ = SpeechBrain_EncoderClassifier.from_hparams(\n",
    "            source=self.embedding,\n",
    "            savedir=f\"{CACHE_DIR}/speechbrain\",\n",
    "            run_opts={\"device\": self.device},\n",
    "            use_auth_token=self.use_auth_token,\n",
    "            revision=self.revision,\n",
    "        )\n",
    "\n",
    "    def to(self, device: torch.device):\n",
    "        if not isinstance(device, torch.device):\n",
    "            raise TypeError(\n",
    "                f\"`device` must be an instance of `torch.device`, got `{type(device).__name__}`\"\n",
    "            )\n",
    "\n",
    "        self.classifier_ = SpeechBrain_EncoderClassifier.from_hparams(\n",
    "            source=self.embedding,\n",
    "            savedir=f\"{CACHE_DIR}/speechbrain\",\n",
    "            run_opts={\"device\": device},\n",
    "            use_auth_token=self.use_auth_token,\n",
    "            revision=self.revision,\n",
    "        )\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    @cached_property\n",
    "    def sample_rate(self) -> int:\n",
    "        return self.classifier_.audio_normalizer.sample_rate\n",
    "\n",
    "    @cached_property\n",
    "    def dimension(self) -> int:\n",
    "        dummy_waveforms = torch.rand(1, 16000).to(self.device)\n",
    "        *_, dimension = self.classifier_.encode_batch(dummy_waveforms).shape\n",
    "        return dimension\n",
    "\n",
    "    @cached_property\n",
    "    def metric(self) -> str:\n",
    "        return \"cosine\"\n",
    "\n",
    "    @cached_property\n",
    "    def min_num_samples(self) -> int:\n",
    "        with torch.inference_mode():\n",
    "            lower, upper = 2, round(0.5 * self.sample_rate)\n",
    "            middle = (lower + upper) // 2\n",
    "            while lower + 1 < upper:\n",
    "                try:\n",
    "                    _ = self.classifier_.encode_batch(\n",
    "                        torch.randn(1, middle).to(self.device)\n",
    "                    )\n",
    "                    upper = middle\n",
    "                except RuntimeError:\n",
    "                    lower = middle\n",
    "\n",
    "                middle = (lower + upper) // 2\n",
    "\n",
    "        return upper\n",
    "\n",
    "    def __call__(\n",
    "        self, waveforms: torch.Tensor, masks: Optional[torch.Tensor] = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        waveforms : (batch_size, num_channels, num_samples)\n",
    "            Only num_channels == 1 is supported.\n",
    "        masks : (batch_size, num_samples), optional\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        embeddings : (batch_size, dimension)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, num_channels, num_samples = waveforms.shape\n",
    "        assert num_channels == 1\n",
    "\n",
    "        waveforms = waveforms.squeeze(dim=1)\n",
    "\n",
    "        if masks is None:\n",
    "            signals = waveforms.squeeze(dim=1)\n",
    "            wav_lens = signals.shape[1] * torch.ones(batch_size)\n",
    "\n",
    "        else:\n",
    "            batch_size_masks, _ = masks.shape\n",
    "            assert batch_size == batch_size_masks\n",
    "\n",
    "            # TODO: speed up the creation of \"signals\"\n",
    "            # preliminary profiling experiments show\n",
    "            # that it accounts for 15% of __call__\n",
    "            # (the remaining 85% being the actual forward pass)\n",
    "\n",
    "            imasks = F.interpolate(\n",
    "                masks.unsqueeze(dim=1), size=num_samples, mode=\"nearest\"\n",
    "            ).squeeze(dim=1)\n",
    "\n",
    "            imasks = imasks > 0.5\n",
    "\n",
    "            signals = pad_sequence(\n",
    "                [\n",
    "                    waveform[imask].contiguous()\n",
    "                    for waveform, imask in zip(waveforms, imasks)\n",
    "                ],\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "            wav_lens = imasks.sum(dim=1)\n",
    "\n",
    "        max_len = wav_lens.max()\n",
    "\n",
    "        # corner case: every signal is too short\n",
    "        if max_len < self.min_num_samples:\n",
    "            return np.NAN * np.zeros((batch_size, self.dimension))\n",
    "\n",
    "        too_short = wav_lens < self.min_num_samples\n",
    "        wav_lens = wav_lens / max_len\n",
    "        wav_lens[too_short] = 1.0\n",
    "\n",
    "        embeddings = (\n",
    "            self.classifier_.encode_batch(signals, wav_lens=wav_lens)\n",
    "            .squeeze(dim=1)\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        embeddings[too_short.cpu().numpy()] = np.NAN\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class ONNXWeSpeakerPretrainedSpeakerEmbedding(BaseInference):\n",
    "    \"\"\"Pretrained WeSpeaker speaker embedding\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding : str\n",
    "        Path to WeSpeaker pretrained speaker embedding\n",
    "    device : torch.device, optional\n",
    "        Device\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    >>> get_embedding = ONNXWeSpeakerPretrainedSpeakerEmbedding(\"hbredin/wespeaker-voxceleb-resnet34-LM\")\n",
    "    >>> assert waveforms.ndim == 3\n",
    "    >>> batch_size, num_channels, num_samples = waveforms.shape\n",
    "    >>> assert num_channels == 1\n",
    "    >>> embeddings = get_embedding(waveforms)\n",
    "    >>> assert embeddings.ndim == 2\n",
    "    >>> assert embeddings.shape[0] == batch_size\n",
    "\n",
    "    >>> assert binary_masks.ndim == 1\n",
    "    >>> assert binary_masks.shape[0] == batch_size\n",
    "    >>> embeddings = get_embedding(waveforms, masks=binary_masks)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding: Text = \"hbredin/wespeaker-voxceleb-resnet34-LM\",\n",
    "        device: Optional[torch.device] = None,\n",
    "    ):\n",
    "        if not ONNX_IS_AVAILABLE:\n",
    "            raise ImportError(\n",
    "                f\"'onnxruntime' must be installed to use '{embedding}' embeddings.\"\n",
    "            )\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if not Path(embedding).exists():\n",
    "            try:\n",
    "                embedding = hf_hub_download(\n",
    "                    repo_id=embedding,\n",
    "                    filename=\"speaker-embedding.onnx\",\n",
    "                )\n",
    "            except RepositoryNotFoundError:\n",
    "                raise ValueError(\n",
    "                    f\"Could not find '{embedding}' on huggingface.co nor on local disk.\"\n",
    "                )\n",
    "\n",
    "        self.embedding = embedding\n",
    "\n",
    "        self.to(device or torch.device(\"cpu\"))\n",
    "\n",
    "    def to(self, device: torch.device):\n",
    "        if not isinstance(device, torch.device):\n",
    "            raise TypeError(\n",
    "                f\"`device` must be an instance of `torch.device`, got `{type(device).__name__}`\"\n",
    "            )\n",
    "\n",
    "        if device.type == \"cpu\":\n",
    "            providers = [\"CPUExecutionProvider\"]\n",
    "        elif device.type == \"cuda\":\n",
    "            providers = [\n",
    "                (\n",
    "                    \"CUDAExecutionProvider\",\n",
    "                    {\n",
    "                        \"cudnn_conv_algo_search\": \"DEFAULT\",  # EXHAUSTIVE / HEURISTIC / DEFAULT\n",
    "                    },\n",
    "                )\n",
    "            ]\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f\"Unsupported device type: {device.type}, falling back to CPU\"\n",
    "            )\n",
    "            device = torch.device(\"cpu\")\n",
    "            providers = [\"CPUExecutionProvider\"]\n",
    "\n",
    "        sess_options = ort.SessionOptions()\n",
    "        sess_options.inter_op_num_threads = 1\n",
    "        sess_options.intra_op_num_threads = 1\n",
    "        self.session_ = ort.InferenceSession(\n",
    "            self.embedding, sess_options=sess_options, providers=providers\n",
    "        )\n",
    "\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    @cached_property\n",
    "    def sample_rate(self) -> int:\n",
    "        return 16000\n",
    "\n",
    "    @cached_property\n",
    "    def dimension(self) -> int:\n",
    "        dummy_waveforms = torch.rand(1, 1, 16000)\n",
    "        features = self.compute_fbank(dummy_waveforms)\n",
    "        embeddings = self.session_.run(\n",
    "            output_names=[\"embs\"], input_feed={\"feats\": features.numpy()}\n",
    "        )[0]\n",
    "        _, dimension = embeddings.shape\n",
    "        return dimension\n",
    "\n",
    "    @cached_property\n",
    "    def metric(self) -> str:\n",
    "        return \"cosine\"\n",
    "\n",
    "    @cached_property\n",
    "    def min_num_samples(self) -> int:\n",
    "        lower, upper = 2, round(0.5 * self.sample_rate)\n",
    "        middle = (lower + upper) // 2\n",
    "        while lower + 1 < upper:\n",
    "            try:\n",
    "                features = self.compute_fbank(torch.randn(1, 1, middle))\n",
    "\n",
    "            except AssertionError:\n",
    "                lower = middle\n",
    "                middle = (lower + upper) // 2\n",
    "                continue\n",
    "\n",
    "            embeddings = self.session_.run(\n",
    "                output_names=[\"embs\"], input_feed={\"feats\": features.numpy()}\n",
    "            )[0]\n",
    "\n",
    "            if np.any(np.isnan(embeddings)):\n",
    "                lower = middle\n",
    "            else:\n",
    "                upper = middle\n",
    "            middle = (lower + upper) // 2\n",
    "\n",
    "        return upper\n",
    "\n",
    "    @cached_property\n",
    "    def min_num_frames(self) -> int:\n",
    "        return self.compute_fbank(torch.randn(1, 1, self.min_num_samples)).shape[1]\n",
    "\n",
    "    def compute_fbank(\n",
    "        self,\n",
    "        waveforms: torch.Tensor,\n",
    "        num_mel_bins: int = 80,\n",
    "        frame_length: int = 25,\n",
    "        frame_shift: int = 10,\n",
    "        dither: float = 0.0,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Extract fbank features\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        waveforms : (batch_size, num_channels, num_samples)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        fbank : (batch_size, num_frames, num_mel_bins)\n",
    "\n",
    "        Source: https://github.com/wenet-e2e/wespeaker/blob/45941e7cba2c3ea99e232d02bedf617fc71b0dad/wespeaker/bin/infer_onnx.py#L30C1-L50\n",
    "        \"\"\"\n",
    "\n",
    "        waveforms = waveforms * (1 << 15)\n",
    "        features = torch.stack(\n",
    "            [\n",
    "                kaldi.fbank(\n",
    "                    waveform,\n",
    "                    num_mel_bins=num_mel_bins,\n",
    "                    frame_length=frame_length,\n",
    "                    frame_shift=frame_shift,\n",
    "                    dither=dither,\n",
    "                    sample_frequency=self.sample_rate,\n",
    "                    window_type=\"hamming\",\n",
    "                    use_energy=False,\n",
    "                )\n",
    "                for waveform in waveforms\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return features - torch.mean(features, dim=1, keepdim=True)\n",
    "\n",
    "    def __call__(\n",
    "        self, waveforms: torch.Tensor, masks: Optional[torch.Tensor] = None\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        waveforms : (batch_size, num_channels, num_samples)\n",
    "            Only num_channels == 1 is supported.\n",
    "        masks : (batch_size, num_samples), optional\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        embeddings : (batch_size, dimension)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, num_channels, num_samples = waveforms.shape\n",
    "        assert num_channels == 1\n",
    "\n",
    "        features = self.compute_fbank(waveforms.to(self.device))\n",
    "        _, num_frames, _ = features.shape\n",
    "\n",
    "        if masks is None:\n",
    "            embeddings = self.session_.run(\n",
    "                output_names=[\"embs\"], input_feed={\"feats\": features.numpy(force=True)}\n",
    "            )[0]\n",
    "\n",
    "            return embeddings\n",
    "\n",
    "        batch_size_masks, _ = masks.shape\n",
    "        assert batch_size == batch_size_masks\n",
    "\n",
    "        imasks = F.interpolate(\n",
    "            masks.unsqueeze(dim=1), size=num_frames, mode=\"nearest\"\n",
    "        ).squeeze(dim=1)\n",
    "\n",
    "        imasks = imasks > 0.5\n",
    "\n",
    "        embeddings = np.NAN * np.zeros((batch_size, self.dimension))\n",
    "\n",
    "        for f, (feature, imask) in enumerate(zip(features, imasks)):\n",
    "            masked_feature = feature[imask]\n",
    "            if masked_feature.shape[0] < self.min_num_frames:\n",
    "                continue\n",
    "\n",
    "            embeddings[f] = self.session_.run(\n",
    "                output_names=[\"embs\"],\n",
    "                input_feed={\"feats\": masked_feature.numpy(force=True)[None]},\n",
    "            )[0][0]\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class PyannoteAudioPretrainedSpeakerEmbedding(BaseInference):\n",
    "    \"\"\"Pretrained pyannote.audio speaker embedding\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding : PipelineModel\n",
    "        pyannote.audio model\n",
    "    device : torch.device, optional\n",
    "        Device\n",
    "    use_auth_token : str, optional\n",
    "        When loading private huggingface.co models, set `use_auth_token`\n",
    "        to True or to a string containing your hugginface.co authentication\n",
    "        token that can be obtained by running `huggingface-cli login`\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    >>> get_embedding = PyannoteAudioPretrainedSpeakerEmbedding(\"pyannote/embedding\")\n",
    "    >>> assert waveforms.ndim == 3\n",
    "    >>> batch_size, num_channels, num_samples = waveforms.shape\n",
    "    >>> assert num_channels == 1\n",
    "    >>> embeddings = get_embedding(waveforms)\n",
    "    >>> assert embeddings.ndim == 2\n",
    "    >>> assert embeddings.shape[0] == batch_size\n",
    "\n",
    "    >>> assert masks.ndim == 1\n",
    "    >>> assert masks.shape[0] == batch_size\n",
    "    >>> embeddings = get_embedding(waveforms, masks=masks)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding: PipelineModel = \"pyannote/embedding\",\n",
    "        device: Optional[torch.device] = None,\n",
    "        use_auth_token: Union[Text, None] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.device = device or torch.device(\"cpu\")\n",
    "\n",
    "        self.model_: Model = get_model(self.embedding, use_auth_token=use_auth_token)\n",
    "        self.model_.eval()\n",
    "        self.model_.to(self.device)\n",
    "\n",
    "    def to(self, device: torch.device):\n",
    "        if not isinstance(device, torch.device):\n",
    "            raise TypeError(\n",
    "                f\"`device` must be an instance of `torch.device`, got `{type(device).__name__}`\"\n",
    "            )\n",
    "\n",
    "        self.model_.to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    @cached_property\n",
    "    def sample_rate(self) -> int:\n",
    "        return self.model_.audio.sample_rate\n",
    "\n",
    "    @cached_property\n",
    "    def dimension(self) -> int:\n",
    "        return self.model_.dimension\n",
    "\n",
    "    @cached_property\n",
    "    def metric(self) -> str:\n",
    "        return \"cosine\"\n",
    "\n",
    "    @cached_property\n",
    "    def min_num_samples(self) -> int:\n",
    "        with torch.inference_mode():\n",
    "            lower, upper = 2, round(0.5 * self.sample_rate)\n",
    "            middle = (lower + upper) // 2\n",
    "            while lower + 1 < upper:\n",
    "                try:\n",
    "                    _ = self.model_(torch.randn(1, 1, middle).to(self.device))\n",
    "                    upper = middle\n",
    "                except Exception:\n",
    "                    lower = middle\n",
    "\n",
    "                middle = (lower + upper) // 2\n",
    "\n",
    "        return upper\n",
    "\n",
    "    def __call__(\n",
    "        self, waveforms: torch.Tensor, masks: Optional[torch.Tensor] = None\n",
    "    ) -> np.ndarray:\n",
    "        with torch.inference_mode():\n",
    "            if masks is None:\n",
    "                embeddings = self.model_(waveforms.to(self.device))\n",
    "            else:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "                    print(\"calling_eb model with, waveforms, masks\",waveforms.size(),masks.size())\n",
    "\n",
    "\n",
    "                    embeddings = self.model_(\n",
    "                        waveforms.to(self.device), weights=masks.to(self.device)\n",
    "                    )\n",
    "        return embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "from distutils.version import LooseVersion\n",
    "from itertools import groupby\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from typeguard import check_argument_types, check_return_type\n",
    "\n",
    "from espnet2.fileio.npy_scp import NpyScpWriter\n",
    "from espnet2.tasks.spk import SpeakerTask\n",
    "from espnet2.torch_utils.device_funcs import to_device\n",
    "from espnet2.torch_utils.set_all_random_seed import set_all_random_seed\n",
    "from espnet2.utils import config_argparse\n",
    "from espnet2.utils.types import str2bool, str2triple_str, str_or_none\n",
    "from espnet.utils.cli_utils import get_commandline_args\n",
    "from pyannote.audio.models.blocks.pooling import StatsPool\n",
    "\n",
    "\n",
    "class Speech2Embedding:\n",
    "    \"\"\"Speech2Embedding class\n",
    "\n",
    "    Examples:\n",
    "        >>> import soundfile\n",
    "        >>> speech2spkembed = Speech2Embedding(\"spk_config.yml\", \"spk.pth\")\n",
    "        >>> audio, rate = soundfile.read(\"speech.wav\")\n",
    "        >>> speech2spkembed(audio)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_config: Union[Path, str] = None,\n",
    "        model_file: Union[Path, str] = None,\n",
    "        device: str = \"cpu\",\n",
    "        dtype: str = \"float32\",\n",
    "        batch_size: int = 1,\n",
    "        min_num_samples: int = 200,\n",
    "        dimension: int = 192,\n",
    "    ):\n",
    "        assert check_argument_types()\n",
    "\n",
    "        spk_model, spk_train_args = SpeakerTask.build_model_from_file(\n",
    "            train_config, model_file, device\n",
    "        )\n",
    "        self.spk_model = spk_model.eval()\n",
    "        self.spk_train_args = spk_train_args\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.batch_size = batch_size\n",
    "        self.min_num_samples = min_num_samples\n",
    "        self.dimension = dimension\n",
    "\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    # def __call__(self, speech: Union[torch.Tensor, np.ndarray],weights: torch.Tensor) -> torch.Tensor:\n",
    "    #     \"\"\"Inference\n",
    "\n",
    "    #     Args:\n",
    "    #         speech: Input speech data\n",
    "\n",
    "    #     Returns:\n",
    "    #         spk_embedding\n",
    "\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     assert check_argument_types()\n",
    "\n",
    "    #     # Input as audio signal\n",
    "    #     if isinstance(speech, np.ndarray):\n",
    "    #         speech = torch.tensor(speech)\n",
    "\n",
    "    #     #(batch_size, 1, num_samples) - > (batch_size, num_samples)\n",
    "\n",
    "    #     #remove the channel dimension\n",
    "    #     if speech.dim() == 3 and speech.size(1) == 1:\n",
    "    #         speech = speech.squeeze(1)\n",
    "\n",
    "    #     print(\"speech size: \", speech.size())\n",
    "        \n",
    "\n",
    "\n",
    "    #     # data: (Nsamples,) -> (1, Nsamples)\n",
    "    #     # speech = speech.unsqueeze(0).to(getattr(torch, self.dtype))\n",
    "    #     logging.info(\"speech length: \" + str(speech.size(1)))\n",
    "    #     batch = {\"speech\": speech, \"extract_embd\": True}\n",
    "\n",
    "    #     # a. To device\n",
    "    #     batch = to_device(batch, device=self.device)\n",
    "\n",
    "    #     print(\"calling speaker model with\",speech.size())\n",
    "    #     print(\"weights size\",weights.size())\n",
    "\n",
    "    #     # b. Forward the model embedding extraction\n",
    "    #     output = self.spk_model(**batch)\n",
    "\n",
    "    #     # features = rearrange(\n",
    "    #     #     features,\n",
    "    #     #     \"batch dimension channel frames -> batch (dimension channel) frames\",\n",
    "    #     # )\n",
    "\n",
    "    #     print(\"output size: \", output.size())\n",
    "       \n",
    "    #     # sequences : (batch, features, frames) torch.Tensor\n",
    "    #     #     Sequences of features.\n",
    "    #     # weights : (batch, frames) or (batch, speakers, frames) torch.Tensor, optional\n",
    "    #     #pooling expect batch, features, frames\n",
    "    #     # return self.stats_pool(output, weights=weights)\n",
    "\n",
    "    #     #we already have pooling impllemented inside the model https://github.com/espnet/espnet/blob/master/espnet2/spk/pooling/chn_attn_stat_pooling.py\n",
    "    #     #id doesent use the weights paramether thoe \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    #     return output\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    #NOTE TAKEN FROM NEMO SPEAKER VERIFICATION THIS JUST SETS WAFEROM SIGNAL THAT IS MASKED TO 0 AND CREATE SIGNALS \n",
    "    # THERE IS A BETTER WAY  it hink BY USING SPEECH LENGHTS BUT OK\n",
    "    def __call__(self, speech: Union[torch.Tensor, np.ndarray], masks: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"Inference\n",
    "\n",
    "        Args:\n",
    "            speech: Input speech data\n",
    "            weights: Input weights\n",
    "            masks: Optional masks for the input speech\n",
    "\n",
    "        Returns:\n",
    "            spk_embedding: Speaker embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        # Input as audio signal\n",
    "        if isinstance(speech, np.ndarray):\n",
    "            speech = torch.tensor(speech)\n",
    "\n",
    "        # Remove the channel dimension if present\n",
    "        if speech.dim() == 3 and speech.size(1) == 1:\n",
    "            speech = speech.squeeze(1)\n",
    "\n",
    "        # print(\"speech size: \", speech.size())\n",
    "\n",
    "        batch_size, num_samples = speech.shape\n",
    "\n",
    "        if masks is None:\n",
    "            signals = speech\n",
    "            wav_lens = signals.shape[1] * torch.ones(batch_size)\n",
    "        else:\n",
    "            batch_size_masks, _ = masks.shape\n",
    "            assert batch_size == batch_size_masks\n",
    "\n",
    "            imasks = F.interpolate(\n",
    "                masks.unsqueeze(dim=1), size=num_samples, mode=\"nearest\"\n",
    "            ).squeeze(dim=1)\n",
    "\n",
    "            imasks = imasks > 0.5\n",
    "\n",
    "            signals = pad_sequence(\n",
    "                [waveform[imask] for waveform, imask in zip(speech, imasks)],\n",
    "                batch_first=True,\n",
    "            )\n",
    "\n",
    "            wav_lens = imasks.sum(dim=1)\n",
    "\n",
    "        max_len = wav_lens.max()\n",
    "\n",
    "        # Corner case: every signal is too short\n",
    "        if max_len < self.min_num_samples:\n",
    "            return np.NAN * np.zeros((batch_size, self.dimension))\n",
    "\n",
    "        too_short = wav_lens < self.min_num_samples\n",
    "        wav_lens[too_short] = max_len\n",
    "\n",
    "        # Prepare batch for the model\n",
    "        batch = {\"speech\": signals.to(self.device), \"extract_embd\": True}\n",
    "        # print(\"calling speaker model with\", signals.size())\n",
    "        # print(\"weights size\", masks.size())\n",
    "\n",
    "        # Forward the model for embedding extraction\n",
    "        output = self.spk_model(**batch)\n",
    "\n",
    "        embeddings = output.cpu().numpy()\n",
    "        embeddings[too_short.cpu().numpy()] = np.NAN\n",
    "\n",
    "        return embeddings\n",
    "    def to(self, device: torch.device, dtype: str = \"float32\"):\n",
    "        \"\"\"Move the model to the device\n",
    "\n",
    "        Args:\n",
    "            device: torch.device\n",
    "            dtype: str\n",
    "\n",
    "        Returns:\n",
    "            self\n",
    "\n",
    "        \"\"\"\n",
    "        print(\"moving model to device\",device)\n",
    "        self.spk_model.to(device)\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(\n",
    "        model_tag: Optional[str] = None,\n",
    "        **kwargs: Optional[Any],\n",
    "    ):\n",
    "        \"\"\"Build Speech2Embedding instance from the pretrained model.\n",
    "\n",
    "        Args:\n",
    "            model_tag (Optional[str]): Model tag of the pretrained models.\n",
    "                Currently, the tags of espnet_model_zoo are supported.\n",
    "\n",
    "        Returns:\n",
    "            Speech2Text: Speech2Embedding instance.\n",
    "\n",
    "        \"\"\"\n",
    "        if model_tag is not None:\n",
    "            try:\n",
    "                from espnet_model_zoo.downloader import ModelDownloader\n",
    "\n",
    "            except ImportError:\n",
    "                logging.error(\n",
    "                    \"`espnet_model_zoo` is not installed. \"\n",
    "                    \"Please install via `pip install -U espnet_model_zoo`.\"\n",
    "                )\n",
    "                raise\n",
    "            d = ModelDownloader()\n",
    "            kwargs.update(**d.download_and_unpack(model_tag))\n",
    "\n",
    "        return Speech2Embedding(**kwargs)\n",
    "\n",
    "\n",
    "class ESPnetSPKSpeakerEmbedding(BaseInference):\n",
    "    \"\"\"Pretrained pyannote.audio speaker embedding\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding : PipelineModel\n",
    "        pyannote.audio model\n",
    "    device : torch.device, optional\n",
    "        Device\n",
    "    use_auth_token : str, optional\n",
    "        When loading private huggingface.co models, set `use_auth_token`\n",
    "        to True or to a string containing your hugginface.co authentication\n",
    "        token that can be obtained by running `huggingface-cli login`\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    >>> get_embedding = PyannoteAudioPretrainedSpeakerEmbedding(\"pyannote/embedding\")\n",
    "    >>> assert waveforms.ndim == 3\n",
    "    >>> batch_size, num_channels, num_samples = waveforms.shape\n",
    "    >>> assert num_channels == 1\n",
    "    >>> embeddings = get_embedding(waveforms)\n",
    "    >>> assert embeddings.ndim == 2\n",
    "    >>> assert embeddings.shape[0] == batch_size\n",
    "\n",
    "    >>> assert masks.ndim == 1\n",
    "    >>> assert masks.shape[0] == batch_size\n",
    "    >>> embeddings = get_embedding(waveforms, masks=masks)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding: str = \"espnet/voxcelebs12_rawnet3\",\n",
    "        device: Optional[torch.device] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.device = \"mps\"\n",
    "\n",
    "        #transform device to string allow\n",
    "\n",
    "        # self.model_: Model = get_model(self.embedding, use_auth_token=use_auth_token)\n",
    "        self.model_ = Speech2Embedding.from_pretrained(model_tag=\"espnet/voxcelebs12_rawnet3\",device=self.device,min_num_samples=self.min_num_samples,dimension=self.dimension)\n",
    "        # self.model_.eval()\n",
    "        # self.model_.to(self.device)\n",
    "\n",
    "    def to(self, device: torch.device):\n",
    "        if not isinstance(device, torch.device):\n",
    "            raise TypeError(\n",
    "                f\"`device` must be an instance of `torch.device`, got `{type(device).__name__}`\"\n",
    "            )\n",
    "\n",
    "        self.model_.to(device)\n",
    "        self.device = device\n",
    "        return self\n",
    "\n",
    "    @cached_property\n",
    "    def sample_rate(self) -> int:\n",
    "        return 16000\n",
    "\n",
    "    @cached_property\n",
    "    def dimension(self) -> int:\n",
    "        return 192\n",
    "\n",
    "    @cached_property\n",
    "    def metric(self) -> str:\n",
    "        return \"cosine\"\n",
    "\n",
    "    @cached_property\n",
    "    def min_num_samples(self) -> int:\n",
    "        with torch.inference_mode():\n",
    "            lower, upper = 2, round(0.5 * self.sample_rate)\n",
    "            middle = (lower + upper) // 2\n",
    "            while lower + 1 < upper:\n",
    "                try:\n",
    "                    _ = self.model_(torch.randn(1, 1, middle).to(self.device))\n",
    "                    upper = middle\n",
    "                except Exception:\n",
    "                    lower = middle\n",
    "\n",
    "                middle = (lower + upper) // 2\n",
    "\n",
    "        return upper\n",
    "\n",
    "    def __call__(\n",
    "        self, waveforms: torch.Tensor, masks: Optional[torch.Tensor] = None\n",
    "    ) -> np.ndarray:\n",
    "        with torch.inference_mode():\n",
    "            if masks is None:\n",
    "                embeddings = self.model_(waveforms.to(self.device))\n",
    "            else:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\")\n",
    "                    embeddings = self.model_(\n",
    "                        waveforms.to(self.device), masks=masks.to(self.device)\n",
    "                    )\n",
    "        return embeddings\n",
    "\n",
    "def PretrainedSpeakerEmbedding(\n",
    "    embedding: PipelineModel,\n",
    "    device: Optional[torch.device] = None,\n",
    "    use_auth_token: Union[Text, None] = None,\n",
    "):\n",
    "    \"\"\"Pretrained speaker embedding\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding : Text\n",
    "        Can be a SpeechBrain (e.g. \"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "        or a pyannote.audio model.\n",
    "    device : torch.device, optional\n",
    "        Device\n",
    "    use_auth_token : str, optional\n",
    "        When loading private huggingface.co models, set `use_auth_token`\n",
    "        to True or to a string containing your hugginface.co authentication\n",
    "        token that can be obtained by running `huggingface-cli login`\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    >>> get_embedding = PretrainedSpeakerEmbedding(\"pyannote/embedding\")\n",
    "    >>> get_embedding = PretrainedSpeakerEmbedding(\"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "    >>> get_embedding = PretrainedSpeakerEmbedding(\"nvidia/speakerverification_en_titanet_large\")\n",
    "    >>> assert waveforms.ndim == 3\n",
    "    >>> batch_size, num_channels, num_samples = waveforms.shape\n",
    "    >>> assert num_channels == 1\n",
    "    >>> embeddings = get_embedding(waveforms)\n",
    "    >>> assert embeddings.ndim == 2\n",
    "    >>> assert embeddings.shape[0] == batch_size\n",
    "\n",
    "    >>> assert masks.ndim == 1\n",
    "    >>> assert masks.shape[0] == batch_size\n",
    "    >>> embeddings = get_embedding(waveforms, masks=masks)\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(embedding, str) and \"pyannote\" in embedding:\n",
    "        return PyannoteAudioPretrainedSpeakerEmbedding(\n",
    "            embedding, device=device, use_auth_token=use_auth_token\n",
    "        )\n",
    "\n",
    "    elif isinstance(embedding, str) and \"speechbrain\" in embedding:\n",
    "        return SpeechBrainPretrainedSpeakerEmbedding(\n",
    "            embedding, device=device, use_auth_token=use_auth_token\n",
    "        )\n",
    "\n",
    "    elif isinstance(embedding, str) and \"nvidia\" in embedding:\n",
    "        return NeMoPretrainedSpeakerEmbedding(embedding, device=device)\n",
    "\n",
    "    elif isinstance(embedding, str) and \"wespeaker\" in embedding:\n",
    "        return ONNXWeSpeakerPretrainedSpeakerEmbedding(embedding, device=device)\n",
    "    \n",
    "    elif isinstance(embedding, str) and \"espnet\" in embedding:\n",
    "        return ESPnetSPKSpeakerEmbedding(embedding, device=device)\n",
    "\n",
    "    else:\n",
    "        # fallback to pyannote in case we are loading a local model\n",
    "        return PyannoteAudioPretrainedSpeakerEmbedding(\n",
    "            embedding, device=device, use_auth_token=use_auth_token\n",
    "        )\n",
    "\n",
    "\n",
    "class SpeakerEmbedding(Pipeline):\n",
    "    \"\"\"Speaker embedding pipeline\n",
    "\n",
    "    This pipeline assumes that each file contains exactly one speaker\n",
    "    and extracts one single embedding from the whole file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    embedding : Model, str, or dict, optional\n",
    "        Pretrained embedding model. Defaults to \"pyannote/embedding\".\n",
    "        See pyannote.audio.pipelines.utils.get_model for supported format.\n",
    "    segmentation : Model, str, or dict, optional\n",
    "        Pretrained segmentation (or voice activity detection) model.\n",
    "        See pyannote.audio.pipelines.utils.get_model for supported format.\n",
    "        Defaults to no voice activity detection.\n",
    "    use_auth_token : str, optional\n",
    "        When loading private huggingface.co models, set `use_auth_token`\n",
    "        to True or to a string containing your hugginface.co authentication\n",
    "        token that can be obtained by running `huggingface-cli login`\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    >>> from pyannote.audio.pipelines import SpeakerEmbedding\n",
    "    >>> pipeline = SpeakerEmbedding()\n",
    "    >>> emb1 = pipeline(\"speaker1.wav\")\n",
    "    >>> emb2 = pipeline(\"speaker2.wav\")\n",
    "    >>> from scipy.spatial.distance import cdist\n",
    "    >>> distance = cdist(emb1, emb2, metric=\"cosine\")[0,0]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding: PipelineModel = \"pyannote/embedding\",\n",
    "        segmentation: Optional[PipelineModel] = None,\n",
    "        use_auth_token: Union[Text, None] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.segmentation = segmentation\n",
    "\n",
    "        self.embedding_model_: Model = get_model(\n",
    "            embedding, use_auth_token=use_auth_token\n",
    "        )\n",
    "\n",
    "        if self.segmentation is not None:\n",
    "            segmentation_model: Model = get_model(\n",
    "                self.segmentation, use_auth_token=use_auth_token\n",
    "            )\n",
    "            self._segmentation = Inference(\n",
    "                segmentation_model,\n",
    "                pre_aggregation_hook=lambda scores: np.max(\n",
    "                    scores, axis=-1, keepdims=True\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    def apply(self, file: AudioFile) -> np.ndarray:\n",
    "        device = self.embedding_model_.device\n",
    "\n",
    "        # read audio file and send it to GPU\n",
    "        waveform = self.embedding_model_.audio(file)[0][None].to(device)\n",
    "\n",
    "        if self.segmentation is None:\n",
    "            weights = None\n",
    "        else:\n",
    "            # obtain voice activity scores\n",
    "            weights = self._segmentation(file).data\n",
    "            # HACK -- this should be fixed upstream\n",
    "            weights[np.isnan(weights)] = 0.0\n",
    "            weights = torch.from_numpy(weights**3)[None, :, 0].to(device)\n",
    "\n",
    "        # extract speaker embedding on parts of\n",
    "        with torch.no_grad():\n",
    "            return self.embedding_model_(waveform, weights=weights).cpu().numpy()\n",
    "\n",
    "\n",
    "def main(\n",
    "    protocol: str = \"VoxCeleb.SpeakerVerification.VoxCeleb1\",\n",
    "    subset: str = \"test\",\n",
    "    embedding: str = \"pyannote/embedding\",\n",
    "    segmentation: Optional[str] = None,\n",
    "):\n",
    "    import typer\n",
    "    from pyannote.database import FileFinder, get_protocol\n",
    "    from pyannote.metrics.binary_classification import det_curve\n",
    "    from scipy.spatial.distance import cdist\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    pipeline = SpeakerEmbedding(embedding=embedding, segmentation=segmentation)\n",
    "\n",
    "    protocol = get_protocol(protocol, preprocessors={\"audio\": FileFinder()})\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    emb = dict()\n",
    "\n",
    "    trials = getattr(protocol, f\"{subset}_trial\")()\n",
    "\n",
    "    for t, trial in enumerate(tqdm(trials)):\n",
    "        audio1 = trial[\"file1\"][\"audio\"]\n",
    "        if audio1 not in emb:\n",
    "            emb[audio1] = pipeline(audio1)\n",
    "\n",
    "        audio2 = trial[\"file2\"][\"audio\"]\n",
    "        if audio2 not in emb:\n",
    "            emb[audio2] = pipeline(audio2)\n",
    "\n",
    "        y_pred.append(cdist(emb[audio1], emb[audio2], metric=\"cosine\")[0][0])\n",
    "        y_true.append(trial[\"reference\"])\n",
    "\n",
    "    _, _, _, eer = det_curve(y_true, np.array(y_pred), distances=True)\n",
    "    typer.echo(\n",
    "        f\"{protocol.name} | {subset} | {embedding} | {segmentation} | EER = {100 * eer:.3f}%\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = PretrainedSpeakerEmbedding(\"espnet/voxcelebs12_rawnet3\", device=\"mps\")\n",
    "# emb1 = pipeline(torch.randn(66, 1, 16000))\n",
    "\n",
    "# print(emb1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MIT License (MIT)\n",
    "#\n",
    "# Copyright (c) 2021- CNRS\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "\"\"\"Speaker diarization pipelines\"\"\"\n",
    "\n",
    "import functools\n",
    "import itertools\n",
    "import math\n",
    "import textwrap\n",
    "import warnings\n",
    "from typing import Callable, Optional, Text, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from pyannote.core import Annotation, SlidingWindowFeature\n",
    "from pyannote.metrics.diarization import GreedyDiarizationErrorRate\n",
    "from pyannote.pipeline.parameter import ParamDict, Uniform\n",
    "\n",
    "from pyannote.audio import Audio, Inference, Model, Pipeline\n",
    "from pyannote.audio.core.io import AudioFile\n",
    "from pyannote.audio.pipelines.clustering import Clustering\n",
    "from pyannote.audio.pipelines.utils import (\n",
    "    PipelineModel,\n",
    "    SpeakerDiarizationMixin,\n",
    "    get_model,\n",
    ")\n",
    "from pyannote.audio.utils.signal import binarize\n",
    "\n",
    "\n",
    "def batchify(iterable, batch_size: int = 32, fillvalue=None):\n",
    "    \"\"\"Batchify iterable\"\"\"\n",
    "    # batchify('ABCDEFG', 3) --> ['A', 'B', 'C']  ['D', 'E', 'F']  [G, ]\n",
    "    args = [iter(iterable)] * batch_size\n",
    "    return itertools.zip_longest(*args, fillvalue=fillvalue)\n",
    "\n",
    "\n",
    "class SpeakerDiarization(SpeakerDiarizationMixin, Pipeline):\n",
    "    \"\"\"Speaker diarization pipeline\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    segmentation : Model, str, or dict, optional\n",
    "        Pretrained segmentation model. Defaults to \"pyannote/segmentation@2022.07\".\n",
    "        See pyannote.audio.pipelines.utils.get_model for supported format.\n",
    "    segmentation_step: float, optional\n",
    "        The segmentation model is applied on a window sliding over the whole audio file.\n",
    "        `segmentation_step` controls the step of this window, provided as a ratio of its\n",
    "        duration. Defaults to 0.1 (i.e. 90% overlap between two consecuive windows).\n",
    "    embedding : Model, str, or dict, optional\n",
    "        Pretrained embedding model. Defaults to \"pyannote/embedding@2022.07\".\n",
    "        See pyannote.audio.pipelines.utils.get_model for supported format.\n",
    "    embedding_exclude_overlap : bool, optional\n",
    "        Exclude overlapping speech regions when extracting embeddings.\n",
    "        Defaults (False) to use the whole speech.\n",
    "    clustering : str, optional\n",
    "        Clustering algorithm. See pyannote.audio.pipelines.clustering.Clustering\n",
    "        for available options. Defaults to \"AgglomerativeClustering\".\n",
    "    segmentation_batch_size : int, optional\n",
    "        Batch size used for speaker segmentation. Defaults to 1.\n",
    "    embedding_batch_size : int, optional\n",
    "        Batch size used for speaker embedding. Defaults to 1.\n",
    "    der_variant : dict, optional\n",
    "        Optimize for a variant of diarization error rate.\n",
    "        Defaults to {\"collar\": 0.0, \"skip_overlap\": False}. This is used in `get_metric`\n",
    "        when instantiating the metric: GreedyDiarizationErrorRate(**der_variant).\n",
    "    use_auth_token : str, optional\n",
    "        When loading private huggingface.co models, set `use_auth_token`\n",
    "        to True or to a string containing your hugginface.co authentication\n",
    "        token that can be obtained by running `huggingface-cli login`\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    # perform (unconstrained) diarization\n",
    "    >>> diarization = pipeline(\"/path/to/audio.wav\")\n",
    "\n",
    "    # perform diarization, targetting exactly 4 speakers\n",
    "    >>> diarization = pipeline(\"/path/to/audio.wav\", num_speakers=4)\n",
    "\n",
    "    # perform diarization, with at least 2 speakers and at most 10 speakers\n",
    "    >>> diarization = pipeline(\"/path/to/audio.wav\", min_speakers=2, max_speakers=10)\n",
    "\n",
    "    # perform diarization and get one representative embedding per speaker\n",
    "    >>> diarization, embeddings = pipeline(\"/path/to/audio.wav\", return_embeddings=True)\n",
    "    >>> for s, speaker in enumerate(diarization.labels()):\n",
    "    ...     # embeddings[s] is the embedding of speaker `speaker`\n",
    "\n",
    "    Hyper-parameters\n",
    "    ----------------\n",
    "    segmentation.threshold\n",
    "    segmentation.min_duration_off\n",
    "    clustering.???\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        segmentation: PipelineModel = \"pyannote/segmentation@2022.07\",\n",
    "        segmentation_step: float = 0.1,\n",
    "        embedding: PipelineModel = \"speechbrain/spkrec-ecapa-voxceleb@5c0be3875fda05e81f3c004ed8c7c06be308de1e\",\n",
    "        embedding_exclude_overlap: bool = False,\n",
    "        clustering: str = \"AgglomerativeClustering\",\n",
    "        embedding_batch_size: int = 1,\n",
    "        segmentation_batch_size: int = 1,\n",
    "        der_variant: Optional[dict] = None,\n",
    "        use_auth_token: Union[Text, None] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.segmentation_model = segmentation\n",
    "        model: Model = get_model(segmentation, use_auth_token=use_auth_token)\n",
    "\n",
    "        self.segmentation_step = segmentation_step\n",
    "\n",
    "        print(\"segmentation_step\", segmentation_step)\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.embedding_batch_size = embedding_batch_size\n",
    "        self.embedding_exclude_overlap = embedding_exclude_overlap\n",
    "\n",
    "        print(\"embedding_batch_size\", embedding_batch_size)\n",
    "\n",
    "        self.klustering = clustering\n",
    "\n",
    "        self.der_variant = der_variant or {\"collar\": 0.0, \"skip_overlap\": False}\n",
    "\n",
    "        segmentation_duration = model.specifications.duration\n",
    "\n",
    "        print(\"segmentation_duration\", segmentation_duration)\n",
    "\n",
    "\n",
    "        self._segmentation = Inference(\n",
    "            model,\n",
    "            duration=segmentation_duration,\n",
    "            step=self.segmentation_step * segmentation_duration,\n",
    "            skip_aggregation=True,\n",
    "            batch_size=segmentation_batch_size,\n",
    "        )\n",
    "\n",
    "        if self._segmentation.model.specifications.powerset:\n",
    "            self.segmentation = ParamDict(\n",
    "                min_duration_off=Uniform(0.0, 1.0),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.segmentation = ParamDict(\n",
    "                threshold=Uniform(0.1, 0.9),\n",
    "                min_duration_off=Uniform(0.0, 1.0),\n",
    "            )\n",
    "\n",
    "        if self.klustering == \"OracleClustering\":\n",
    "            metric = \"not_applicable\"\n",
    "\n",
    "        else:\n",
    "            self._embedding = PretrainedSpeakerEmbedding(\n",
    "                self.embedding, use_auth_token=use_auth_token\n",
    "            )\n",
    "            self._audio = Audio(sample_rate=self._embedding.sample_rate, mono=\"downmix\")\n",
    "            metric = self._embedding.metric\n",
    "\n",
    "        try:\n",
    "            Klustering = Clustering[clustering]\n",
    "        except KeyError:\n",
    "            raise ValueError(\n",
    "                f'clustering must be one of [{\", \".join(list(Clustering.__members__))}]'\n",
    "            )\n",
    "        self.clustering = Klustering.value(metric=metric)\n",
    "\n",
    "    @property\n",
    "    def segmentation_batch_size(self) -> int:\n",
    "        return self._segmentation.batch_size\n",
    "\n",
    "    @segmentation_batch_size.setter\n",
    "    def segmentation_batch_size(self, batch_size: int):\n",
    "        self._segmentation.batch_size = batch_size\n",
    "\n",
    "    def default_parameters(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def classes(self):\n",
    "        speaker = 0\n",
    "        while True:\n",
    "            yield f\"SPEAKER_{speaker:02d}\"\n",
    "            speaker += 1\n",
    "\n",
    "    @property\n",
    "    def CACHED_SEGMENTATION(self):\n",
    "        return \"training_cache/segmentation\"\n",
    "\n",
    "    def get_segmentations(self, file, hook=None) -> SlidingWindowFeature:\n",
    "        \"\"\"Apply segmentation model\n",
    "\n",
    "        Parameter\n",
    "        ---------\n",
    "        file : AudioFile\n",
    "        hook : Optional[Callable]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        segmentations : (num_chunks, num_frames, num_speakers) SlidingWindowFeature\n",
    "        \"\"\"\n",
    "\n",
    "        if hook is not None:\n",
    "            hook = functools.partial(hook, \"segmentation\", None)\n",
    "\n",
    "        if self.training:\n",
    "            if self.CACHED_SEGMENTATION in file:\n",
    "                segmentations = file[self.CACHED_SEGMENTATION]\n",
    "            else:\n",
    "                segmentations = self._segmentation(file, hook=hook)\n",
    "                file[self.CACHED_SEGMENTATION] = segmentations\n",
    "        else:\n",
    "            segmentations: SlidingWindowFeature = self._segmentation(file, hook=hook)\n",
    "\n",
    "        return segmentations\n",
    "\n",
    "    def get_embeddings(\n",
    "        self,\n",
    "        file,\n",
    "        binary_segmentations: SlidingWindowFeature,\n",
    "        exclude_overlap: bool = False,\n",
    "        hook: Optional[Callable] = None,\n",
    "    ):\n",
    "        \"\"\"Extract embeddings for each (chunk, speaker) pair\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file : AudioFile\n",
    "        binary_segmentations : (num_chunks, num_frames, num_speakers) SlidingWindowFeature\n",
    "            Binarized segmentation.\n",
    "        exclude_overlap : bool, optional\n",
    "            Exclude overlapping speech regions when extracting embeddings.\n",
    "            In case non-overlapping speech is too short, use the whole speech.\n",
    "        hook: Optional[Callable]\n",
    "            Called during embeddings after every batch to report the progress\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        embeddings : (num_chunks, num_speakers, dimension) array\n",
    "        \"\"\"\n",
    "\n",
    "        # when optimizing the hyper-parameters of this pipeline with frozen\n",
    "        # \"segmentation.threshold\", one can reuse the embeddings from the first trial,\n",
    "        # bringing a massive speed up to the optimization process (and hence allowing to use\n",
    "        # a larger search space).\n",
    "        if self.training:\n",
    "            # we only re-use embeddings if they were extracted based on the same value of the\n",
    "            # \"segmentation.threshold\" hyperparameter or if the segmentation model relies on\n",
    "            # `powerset` mode\n",
    "            cache = file.get(\"training_cache/embeddings\", dict())\n",
    "            if (\"embeddings\" in cache) and (\n",
    "                self._segmentation.model.specifications.powerset\n",
    "                or (cache[\"segmentation.threshold\"] == self.segmentation.threshold)\n",
    "            ):\n",
    "                return cache[\"embeddings\"]\n",
    "\n",
    "        duration = binary_segmentations.sliding_window.duration\n",
    "        num_chunks, num_frames, num_speakers = binary_segmentations.data.shape\n",
    "\n",
    "        if exclude_overlap:\n",
    "            # minimum number of samples needed to extract an embedding\n",
    "            # (a lower number of samples would result in an error)\n",
    "            min_num_samples = self._embedding.min_num_samples\n",
    "\n",
    "            # corresponding minimum number of frames\n",
    "            num_samples = duration * self._embedding.sample_rate\n",
    "            min_num_frames = math.ceil(num_frames * min_num_samples / num_samples)\n",
    "\n",
    "            # zero-out frames with overlapping speech\n",
    "            clean_frames = 1.0 * (\n",
    "                np.sum(binary_segmentations.data, axis=2, keepdims=True) < 2\n",
    "            )\n",
    "            clean_segmentations = SlidingWindowFeature(\n",
    "                binary_segmentations.data * clean_frames,\n",
    "                binary_segmentations.sliding_window,\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            min_num_frames = -1\n",
    "            clean_segmentations = SlidingWindowFeature(\n",
    "                binary_segmentations.data, binary_segmentations.sliding_window\n",
    "            )\n",
    "\n",
    "        def iter_waveform_and_mask():\n",
    "            for (chunk, masks), (_, clean_masks) in zip(\n",
    "                binary_segmentations, clean_segmentations\n",
    "            ):\n",
    "                # chunk: Segment(t, t + duration)\n",
    "                # masks: (num_frames, local_num_speakers) np.ndarray\n",
    "\n",
    "                waveform, _ = self._audio.crop(\n",
    "                    file,\n",
    "                    chunk,\n",
    "                    duration=duration,\n",
    "                    mode=\"pad\",\n",
    "                )\n",
    "                # waveform: (1, num_samples) torch.Tensor\n",
    "\n",
    "                # mask may contain NaN (in case of partial stitching)\n",
    "                masks = np.nan_to_num(masks, nan=0.0).astype(np.float32)\n",
    "                clean_masks = np.nan_to_num(clean_masks, nan=0.0).astype(np.float32)\n",
    "\n",
    "                for mask, clean_mask in zip(masks.T, clean_masks.T):\n",
    "                    # mask: (num_frames, ) np.ndarray\n",
    "\n",
    "                    if np.sum(clean_mask) > min_num_frames:\n",
    "                        used_mask = clean_mask\n",
    "                    else:\n",
    "                        used_mask = mask\n",
    "\n",
    "                    yield waveform[None], torch.from_numpy(used_mask)[None]\n",
    "                    # w: (1, 1, num_samples) torch.Tensor\n",
    "                    # m: (1, num_frames) torch.Tensor\n",
    "\n",
    "        batches = batchify(\n",
    "            iter_waveform_and_mask(),\n",
    "            batch_size=self.embedding_batch_size,\n",
    "            fillvalue=(None, None),\n",
    "        )\n",
    "\n",
    "        batch_count = math.ceil(num_chunks * num_speakers / self.embedding_batch_size)\n",
    "\n",
    "        embedding_batches = []\n",
    "\n",
    "        if hook is not None:\n",
    "            hook(\"embeddings\", None, total=batch_count, completed=0)\n",
    "\n",
    "        for i, batch in enumerate(batches, 1):\n",
    "            waveforms, masks = zip(*filter(lambda b: b[0] is not None, batch))\n",
    "\n",
    "            waveform_batch = torch.vstack(waveforms)\n",
    "            # (batch_size, 1, num_samples) torch.Tensor\n",
    "\n",
    "            mask_batch = torch.vstack(masks)\n",
    "            # (batch_size, num_frames) torch.Tensor\n",
    "\n",
    "            embedding_batch: np.ndarray = self._embedding(\n",
    "                waveform_batch, masks=mask_batch\n",
    "            )\n",
    "            # (batch_size, dimension) np.ndarray\n",
    "\n",
    "            embedding_batches.append(embedding_batch)\n",
    "\n",
    "            if hook is not None:\n",
    "                hook(\"embeddings\", embedding_batch, total=batch_count, completed=i)\n",
    "\n",
    "        embedding_batches = np.vstack(embedding_batches)\n",
    "\n",
    "        embeddings = rearrange(embedding_batches, \"(c s) d -> c s d\", c=num_chunks)\n",
    "\n",
    "        print(\"embeddings.shape\", embeddings.shape,\"num_chunks\", num_chunks)\n",
    "\n",
    "        # caching embeddings for subsequent trials\n",
    "        # (see comments at the top of this method for more details)\n",
    "        if self.training:\n",
    "            if self._segmentation.model.specifications.powerset:\n",
    "                file[\"training_cache/embeddings\"] = {\n",
    "                    \"embeddings\": embeddings,\n",
    "                }\n",
    "            else:\n",
    "                file[\"training_cache/embeddings\"] = {\n",
    "                    \"segmentation.threshold\": self.segmentation.threshold,\n",
    "                    \"embeddings\": embeddings,\n",
    "                }\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def reconstruct(\n",
    "        self,\n",
    "        segmentations: SlidingWindowFeature,\n",
    "        hard_clusters: np.ndarray,\n",
    "        count: SlidingWindowFeature,\n",
    "    ) -> SlidingWindowFeature:\n",
    "        \"\"\"Build final discrete diarization out of clustered segmentation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        segmentations : (num_chunks, num_frames, num_speakers) SlidingWindowFeature\n",
    "            Raw speaker segmentation.\n",
    "        hard_clusters : (num_chunks, num_speakers) array\n",
    "            Output of clustering step.\n",
    "        count : (total_num_frames, 1) SlidingWindowFeature\n",
    "            Instantaneous number of active speakers.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        discrete_diarization : SlidingWindowFeature\n",
    "            Discrete (0s and 1s) diarization.\n",
    "        \"\"\"\n",
    "\n",
    "        num_chunks, num_frames, local_num_speakers = segmentations.data.shape\n",
    "\n",
    "        num_clusters = np.max(hard_clusters) + 1\n",
    "        clustered_segmentations = np.NAN * np.zeros(\n",
    "            (num_chunks, num_frames, num_clusters)\n",
    "        )\n",
    "\n",
    "        for c, (cluster, (chunk, segmentation)) in enumerate(\n",
    "            zip(hard_clusters, segmentations)\n",
    "        ):\n",
    "            # cluster is (local_num_speakers, )-shaped\n",
    "            # segmentation is (num_frames, local_num_speakers)-shaped\n",
    "            for k in np.unique(cluster):\n",
    "                if k == -2:\n",
    "                    continue\n",
    "\n",
    "                # TODO: can we do better than this max here?\n",
    "                clustered_segmentations[c, :, k] = np.max(\n",
    "                    segmentation[:, cluster == k], axis=1\n",
    "                )\n",
    "\n",
    "        clustered_segmentations = SlidingWindowFeature(\n",
    "            clustered_segmentations, segmentations.sliding_window\n",
    "        )\n",
    "\n",
    "        return self.to_diarization(clustered_segmentations, count)\n",
    "\n",
    "    def apply(\n",
    "        self,\n",
    "        file: AudioFile,\n",
    "        num_speakers: Optional[int] = None,\n",
    "        min_speakers: Optional[int] = None,\n",
    "        max_speakers: Optional[int] = None,\n",
    "        return_embeddings: bool = False,\n",
    "        hook: Optional[Callable] = None,\n",
    "    ) -> Annotation:\n",
    "        \"\"\"Apply speaker diarization\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        file : AudioFile\n",
    "            Processed file.\n",
    "        num_speakers : int, optional\n",
    "            Number of speakers, when known.\n",
    "        min_speakers : int, optional\n",
    "            Minimum number of speakers. Has no effect when `num_speakers` is provided.\n",
    "        max_speakers : int, optional\n",
    "            Maximum number of speakers. Has no effect when `num_speakers` is provided.\n",
    "        return_embeddings : bool, optional\n",
    "            Return representative speaker embeddings.\n",
    "        hook : callable, optional\n",
    "            Callback called after each major steps of the pipeline as follows:\n",
    "                hook(step_name,      # human-readable name of current step\n",
    "                     step_artefact,  # artifact generated by current step\n",
    "                     file=file)      # file being processed\n",
    "            Time-consuming steps call `hook` multiple times with the same `step_name`\n",
    "            and additional `completed` and `total` keyword arguments usable to track\n",
    "            progress of current step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        diarization : Annotation\n",
    "            Speaker diarization\n",
    "        embeddings : np.array, optional\n",
    "            Representative speaker embeddings such that `embeddings[i]` is the\n",
    "            speaker embedding for i-th speaker in diarization.labels().\n",
    "            Only returned when `return_embeddings` is True.\n",
    "        \"\"\"\n",
    "\n",
    "        # setup hook (e.g. for debugging purposes)\n",
    "        hook = self.setup_hook(file, hook=hook)\n",
    "\n",
    "        num_speakers, min_speakers, max_speakers = self.set_num_speakers(\n",
    "            num_speakers=num_speakers,\n",
    "            min_speakers=min_speakers,\n",
    "            max_speakers=max_speakers,\n",
    "        )\n",
    "\n",
    "        segmentations = self.get_segmentations(file, hook=hook)\n",
    "        hook(\"segmentation\", segmentations)\n",
    "        #   shape: (num_chunks, num_frames, local_num_speakers)\n",
    "        num_chunks, num_frames, local_num_speakers = segmentations.data.shape\n",
    "\n",
    "        print(\"num_chunks\", num_chunks)\n",
    "\n",
    "        # binarize segmentation\n",
    "        if self._segmentation.model.specifications.powerset:\n",
    "            binarized_segmentations = segmentations\n",
    "        else:\n",
    "            binarized_segmentations: SlidingWindowFeature = binarize(\n",
    "                segmentations,\n",
    "                onset=self.segmentation.threshold,\n",
    "                initial_state=False,\n",
    "            )\n",
    "\n",
    "        hook(\"binarized_segmentation\", binarized_segmentations)\n",
    "        # estimate frame-level number of instantaneous speakers\n",
    "        count = self.speaker_count(\n",
    "            binarized_segmentations,\n",
    "            self._segmentation.model.receptive_field,\n",
    "            warm_up=(0.0, 0.0),\n",
    "        )\n",
    "        hook(\"speaker_counting\", count)\n",
    "        #   shape: (num_frames, 1)\n",
    "        #   dtype: int\n",
    "\n",
    "        # exit early when no speaker is ever active\n",
    "        if np.nanmax(count.data) == 0.0:\n",
    "            diarization = Annotation(uri=file[\"uri\"])\n",
    "            if return_embeddings:\n",
    "                return diarization, np.zeros((0, self._embedding.dimension))\n",
    "\n",
    "            return diarization\n",
    "\n",
    "        # skip speaker embedding extraction and clustering when only one speaker\n",
    "        if not return_embeddings and max_speakers < 2:\n",
    "            hard_clusters = np.zeros((num_chunks, local_num_speakers), dtype=np.int8)\n",
    "            embeddings = None\n",
    "            centroids = None\n",
    "\n",
    "        else:\n",
    "\n",
    "            # skip speaker embedding extraction with oracle clustering\n",
    "            if self.klustering == \"OracleClustering\" and not return_embeddings:\n",
    "                embeddings = None\n",
    "\n",
    "            else:\n",
    "                embeddings = self.get_embeddings(\n",
    "                    file,\n",
    "                    binarized_segmentations,\n",
    "                    exclude_overlap=self.embedding_exclude_overlap,\n",
    "                    hook=hook,\n",
    "                )\n",
    "                hook(\"embeddings\", embeddings)\n",
    "                #   shape: (num_chunks, local_num_speakers, dimension)\n",
    "\n",
    "            hard_clusters, _, centroids = self.clustering(\n",
    "                embeddings=embeddings,\n",
    "                segmentations=binarized_segmentations,\n",
    "                num_clusters=num_speakers,\n",
    "                min_clusters=min_speakers,\n",
    "                max_clusters=max_speakers,\n",
    "                file=file,  # <== for oracle clustering\n",
    "                frames=self._segmentation.model.receptive_field,  # <== for oracle clustering\n",
    "            )\n",
    "            # hook(\"hard_clusters\", hard_clusters)\n",
    "            # hard_clusters: (num_chunks, num_speakers)\n",
    "            # centroids: (num_speakers, dimension)\n",
    "\n",
    "        # number of detected clusters is the number of different speakers\n",
    "        num_different_speakers = np.max(hard_clusters) + 1\n",
    "\n",
    "        # detected number of speakers can still be out of bounds\n",
    "        # (specifically, lower than `min_speakers`), since there could be too few embeddings\n",
    "        # to make enough clusters with a given minimum cluster size.\n",
    "        if (\n",
    "            num_different_speakers < min_speakers\n",
    "            or num_different_speakers > max_speakers\n",
    "        ):\n",
    "            warnings.warn(\n",
    "                textwrap.dedent(\n",
    "                    f\"\"\"\n",
    "                The detected number of speakers ({num_different_speakers}) is outside\n",
    "                the given bounds [{min_speakers}, {max_speakers}]. This can happen if the\n",
    "                given audio file is too short to contain {min_speakers} or more speakers.\n",
    "                Try to lower the desired minimal number of speakers.\n",
    "                \"\"\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # during counting, we could possibly overcount the number of instantaneous\n",
    "        # speakers due to segmentation errors, so we cap the maximum instantaneous number\n",
    "        # of speakers by the `max_speakers` value\n",
    "        count.data = np.minimum(count.data, max_speakers).astype(np.int8)\n",
    "\n",
    "        # reconstruct discrete diarization from raw hard clusters\n",
    "\n",
    "        # keep track of inactive speakers\n",
    "        inactive_speakers = np.sum(binarized_segmentations.data, axis=1) == 0\n",
    "        #   shape: (num_chunks, num_speakers)\n",
    "\n",
    "        hard_clusters[inactive_speakers] = -2\n",
    "\n",
    "        hook(\"hard_clusters\", hard_clusters)\n",
    "\n",
    "        discrete_diarization = self.reconstruct(\n",
    "            segmentations,\n",
    "            hard_clusters,\n",
    "            count,\n",
    "        )\n",
    "        hook(\"discrete_diarization\", discrete_diarization)\n",
    "\n",
    "        # convert to continuous diarization\n",
    "        diarization = self.to_annotation(\n",
    "            discrete_diarization,\n",
    "            min_duration_on=0.0,\n",
    "            min_duration_off=self.segmentation.min_duration_off,\n",
    "        )\n",
    "        diarization.uri = file[\"uri\"]\n",
    "\n",
    "        # at this point, `diarization` speaker labels are integers\n",
    "        # from 0 to `num_speakers - 1`, aligned with `centroids` rows.\n",
    "\n",
    "        if \"annotation\" in file and file[\"annotation\"]:\n",
    "            # when reference is available, use it to map hypothesized speakers\n",
    "            # to reference speakers (this makes later error analysis easier\n",
    "            # but does not modify the actual output of the diarization pipeline)\n",
    "            _, mapping = self.optimal_mapping(\n",
    "                file[\"annotation\"], diarization, return_mapping=True\n",
    "            )\n",
    "\n",
    "            # in case there are more speakers in the hypothesis than in\n",
    "            # the reference, those extra speakers are missing from `mapping`.\n",
    "            # we add them back here\n",
    "            mapping = {key: mapping.get(key, key) for key in diarization.labels()}\n",
    "\n",
    "        else:\n",
    "            # when reference is not available, rename hypothesized speakers\n",
    "            # to human-readable SPEAKER_00, SPEAKER_01, ...\n",
    "            mapping = {\n",
    "                label: expected_label\n",
    "                for label, expected_label in zip(diarization.labels(), self.classes())\n",
    "            }\n",
    "\n",
    "        diarization = diarization.rename_labels(mapping=mapping)\n",
    "\n",
    "        # at this point, `diarization` speaker labels are strings (or mix of\n",
    "        # strings and integers when reference is available and some hypothesis\n",
    "        # speakers are not present in the reference)\n",
    "\n",
    "        if not return_embeddings:\n",
    "            return diarization\n",
    "\n",
    "        # this can happen when we use OracleClustering\n",
    "        if centroids is None:\n",
    "            return diarization, None\n",
    "\n",
    "        # The number of centroids may be smaller than the number of speakers\n",
    "        # in the annotation. This can happen if the number of active speakers\n",
    "        # obtained from `speaker_count` for some frames is larger than the number\n",
    "        # of clusters obtained from `clustering`. In this case, we append zero embeddings\n",
    "        # for extra speakers\n",
    "        if len(diarization.labels()) > centroids.shape[0]:\n",
    "            centroids = np.pad(\n",
    "                centroids, ((0, len(diarization.labels()) - centroids.shape[0]), (0, 0))\n",
    "            )\n",
    "\n",
    "        # re-order centroids so that they match\n",
    "        # the order given by diarization.labels()\n",
    "        inverse_mapping = {label: index for index, label in mapping.items()}\n",
    "        centroids = centroids[\n",
    "            [inverse_mapping[label] for label in diarization.labels()]\n",
    "        ]\n",
    "\n",
    "        return diarization, centroids\n",
    "\n",
    "    def get_metric(self) -> GreedyDiarizationErrorRate:\n",
    "        return GreedyDiarizationErrorRate(**self.der_variant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL EMBED TRY\n",
    "\n",
    "\n",
    "import pickle\n",
    "import einops\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import yaml\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "from typing import Any, Mapping, Optional, Text\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from rich.progress import Progress, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn\n",
    "from pyannote.audio.pipelines.utils.hook import ArtifactHook,ProgressHook\n",
    "\n",
    "class CombinedHook:\n",
    "    \"\"\"Composite Hook to save artifacts and show progress of each internal step.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    artifacts: list of str, optional\n",
    "        List of steps to save. Defaults to all steps.\n",
    "    file_key: str, optional\n",
    "        Key used to store artifacts in `file`.\n",
    "        Defaults to \"artifact\".\n",
    "    transient: bool, optional\n",
    "        Clear the progress on exit. Defaults to False.\n",
    "\n",
    "    Usage\n",
    "    -----\n",
    "    >>> with CombinedHook() as hook:\n",
    "    ...     output = pipeline(file, hook=hook)\n",
    "    # file[\"artifact\"] contains a dict with artifacts of each step\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *artifacts, file_key: str = \"artifact\", transient: bool = False):\n",
    "        self.artifact_hook = ArtifactHook(*artifacts, file_key=file_key)\n",
    "        self.progress_hook = ProgressHook(transient=transient)\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.artifact_hook.__enter__()\n",
    "        self.progress_hook.__enter__()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.artifact_hook.__exit__(*args)\n",
    "        self.progress_hook.__exit__(*args)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        step_name: Text,\n",
    "        step_artifact: Any,\n",
    "        file: Optional[Mapping] = None,\n",
    "        total: Optional[int] = None,\n",
    "        completed: Optional[int] = None,\n",
    "    ):\n",
    "        self.artifact_hook(step_name, step_artifact, file, total, completed)\n",
    "        self.progress_hook(step_name, step_artifact, file, total, completed)\n",
    "\n",
    "\n",
    "# #perform speaker diarization on full audio\n",
    "    # pipeline = Pipeline.from_pretrained(\n",
    "    # \"./config2.yaml\",\n",
    "    #     use_auth_token=\"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\")\n",
    "\n",
    "#     pipeline:\n",
    "#   name: pyannote.audio.pipelines.SpeakerDiarization\n",
    "#   params:\n",
    "#     clustering: AgglomerativeClustering\n",
    "#     embedding: pyannote/embedding\n",
    "#     embedding_batch_size: 32\n",
    "#     embedding_exclude_overlap: true\n",
    "#     segmentation: pyannote/segmentation-3.0\n",
    "#     segmentation_batch_size: 32\n",
    "\n",
    "# params:\n",
    "#   clustering:\n",
    "#     method: centroid\n",
    "#     min_cluster_size: 12\n",
    "#     threshold: 0.7045654963945799\n",
    "#   segmentation:\n",
    "#     min_duration_off: 0.0\n",
    "\n",
    "\n",
    "    # pipeline = SpeakerDiarization(  \n",
    "    #     segmentation =  \"pyannote/segmentation@2022.07\",\n",
    "    #     segmentation_step= 0.1,\n",
    "    #     embedding: PipelineModel = \"speechbrain/spkrec-ecapa-voxceleb@5c0be3875fda05e81f3c004ed8c7c06be308de1e\",\n",
    "    #     embedding_exclude_overlap: bool = False,\n",
    "    #     clustering: str = \"AgglomerativeClustering\",\n",
    "    #     embedding_batch_size: int = 1,\n",
    "    #     segmentation_batch_size: int = 1,\n",
    "    #     der_variant: Optional[dict] = None,\n",
    "    #     use_auth_token: Union[Text, None] = None,\n",
    "\n",
    "pipeline = SpeakerDiarization(\n",
    "    segmentation =  \"pyannote/segmentation-3.0\",\n",
    "    segmentation_step= 0.1,\n",
    "    embedding = \"espnet/voxcelebs12_rawnet3\",\n",
    "    embedding_exclude_overlap = True,\n",
    "    clustering = \"AgglomerativeClustering\",\n",
    "    embedding_batch_size = 32,\n",
    "    segmentation_batch_size = 32,\n",
    "    der_variant = None,\n",
    "    use_auth_token = \"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\"\n",
    ")\n",
    "\n",
    "pipeline.to(torch.device(\"mps\"))\n",
    "\n",
    "cc = \"\"\"\n",
    "params:\n",
    "    clustering:\n",
    "        method: centroid\n",
    "        min_cluster_size: 12\n",
    "        threshold: 0.7045654963945799\n",
    "    segmentation:\n",
    "        min_duration_off: 0.0\n",
    "\"\"\"\n",
    "\n",
    "#TODO FIND THE RIGHT SEGMENTATION THRESHOLD THAT IS DEFAULT!!!!!!!!  0.5 IS GUESEING \n",
    "#note segmentation.threshold not used model is powerset it dpesent mater since segmentation meodel is that one\n",
    "\n",
    "\n",
    "cc = yaml.load(cc, Loader=yaml.FullLoader)\n",
    "\n",
    "print(cc)\n",
    "\n",
    "# pipeline.instantiate(config[\"params\"])\n",
    "pipeline.instantiate(cc[\"params\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(\"outputttt3_final_output.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "\n",
    "\n",
    "all_full_embedings = []\n",
    "all_hard_clusters = []\n",
    "all_diars = []\n",
    "all_centroids = []\n",
    "for i in range(data.shape[0]):\n",
    "\n",
    "    mic = data[i]\n",
    "\n",
    "\n",
    "    sr = 16000\n",
    "\n",
    "    print(mic.shape)\n",
    "\n",
    "    #(30, 6, 160000) -> (6, 160000 * 30)\n",
    "\n",
    "    mic = einops.rearrange(mic, \"a b c -> b (a c)\")\n",
    "\n",
    "    print(mic.shape) \n",
    "\n",
    "\n",
    "    #(6, 4800000) -> (14800000 * 6)\n",
    "\n",
    "    mic_full_flat = einops.rearrange(mic, \"a b -> (a b)\")\n",
    "\n",
    "    print(mic_full_flat.shape) #(28800000,)\n",
    "\n",
    "\n",
    "    # audio_in_memory = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "    # type(waveform)=<class 'torch.Tensor'>\n",
    "    # waveform.shape=torch.Size([1, 480000])\n",
    "    # waveform.dtype=torch.float32\n",
    "\n",
    "    audio_in_memory = {\"waveform\": torch.from_numpy(mic_full_flat).unsqueeze(0), \"sample_rate\": 16000}\n",
    "\n",
    "    \n",
    "\n",
    "    # run the pipeline on an audio file\n",
    "\n",
    "\n",
    "    with CombinedHook() as hook:\n",
    "\n",
    "        diarization,embedings = pipeline(audio_in_memory,hook=hook,return_embeddings=True)\n",
    "\n",
    "\n",
    "    print(audio_in_memory.keys())\n",
    "\n",
    "    full_embedings = audio_in_memory[\"artifact\"][\"embeddings\"] #(num_chunks, local_num_speakers, dimension)\n",
    "    hard_clusters = audio_in_memory[\"artifact\"][\"hard_clusters\"] #(num_chunks, local_num_speakers)\n",
    "\n",
    "    all_full_embedings.append(full_embedings)\n",
    "    all_hard_clusters.append(hard_clusters)\n",
    "    all_diars.append(diarization)\n",
    "    all_centroids.append(embedings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_max_amplitude = []\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    mic = data[i]\n",
    "\n",
    "    sr = 16000\n",
    "\n",
    "    current_diar = all_diars[i]\n",
    "\n",
    "    current_mic = mic\n",
    "\n",
    "    current_mic = einops.rearrange(current_mic, \"a b c -> b (a c)\")\n",
    "\n",
    "    current_mic = einops.rearrange(current_mic, \"a b -> (a b)\")\n",
    "\n",
    "    \n",
    "\n",
    "    current_speaker_max_amplitude = {}\n",
    "\n",
    "    for turn, _, speaker in current_diar.itertracks(yield_label=True):\n",
    "            \n",
    "            start = int(turn.start * sr)\n",
    "    \n",
    "            end = int(turn.end * sr)\n",
    "    \n",
    "            segment = current_mic[start:end]\n",
    "    \n",
    "            segment = np.array(segment)\n",
    "    \n",
    "            max_amplitude = np.max(np.abs(segment))\n",
    "    \n",
    "            if speaker not in current_speaker_max_amplitude:\n",
    "    \n",
    "                current_speaker_max_amplitude[speaker] = 0\n",
    "    \n",
    "            current_speaker_max_amplitude[speaker] = max(max_amplitude,current_speaker_max_amplitude[speaker])\n",
    "\n",
    "    current_speaker_max_amplitude = dict(sorted(current_speaker_max_amplitude.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    all_max_amplitude.append(current_speaker_max_amplitude)\n",
    "\n",
    "print(all_max_amplitude)\n",
    "\n",
    "#get centroids\n",
    "\n",
    "\n",
    "#plot histrograms for each run \n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(len(all_max_amplitude)):\n",
    "    current_max_amplitude = all_max_amplitude[i]\n",
    "    for speaker, amplitude in current_max_amplitude.items():\n",
    "        fig.add_trace(go.Bar(x=[\n",
    "            f\"mic_{i}_{speaker}\"], y=[amplitude]))\n",
    "        \n",
    "fig.update_layout(yaxis_title=\"Amplitude\", xaxis_title=\"Speaker\", title=\"Speaker Amplitude\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print most probable spekaer for each run based on amplitude\n",
    "\n",
    "best_speakers_per_mic = []\n",
    "for i in range(len(all_max_amplitude)):\n",
    "    current_max_amplitude = all_max_amplitude[i]\n",
    "    print(f\"mic_{i} best speaker {list(current_max_amplitude.keys())[0]} with max amplitude {list(current_max_amplitude.values())[0]}\")\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot all centroids on umap\n",
    "\n",
    "centroid_speaker_labels_run = []\n",
    "\n",
    "runsss= []\n",
    "sizes  = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(all_centroids)):\n",
    "\n",
    "    current_centroids = all_centroids[i]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #sorted by speaker automatically\n",
    "    for j in range(current_centroids.shape[0]):\n",
    "\n",
    "        runsss.append(i)\n",
    "\n",
    "        centroid_speaker_labels_run.append(f\"mic_{i}_speaker_{j}\")\n",
    "\n",
    "        speaker_key = f\"SPEAKER_{j:02d}\"\n",
    "        sizes.append(all_max_amplitude[i].get(speaker_key, 2222))  # Default size to 1 if key not found\n",
    "\n",
    "        # sizes.append(all_max_amplitude[i][f\"SPEAKER_0{j}\"])\n",
    "\n",
    "print(centroid_speaker_labels_run)\n",
    "\n",
    "all_centroids_flat = np.concatenate(all_centroids,axis=0)\n",
    "\n",
    "print(all_centroids_flat.shape)\n",
    "\n",
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(metric=\"cosine\",n_components=3)\n",
    "\n",
    "umap_centroids = reducer.fit_transform(all_centroids_flat)\n",
    "\n",
    "#plotly\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(umap_centroids, columns=[\"x\", \"y\",\"z\"])\n",
    "#\n",
    "df[\"speaker\"] = centroid_speaker_labels_run\n",
    "\n",
    "#to string \n",
    "\n",
    "df[\"run\"] = runsss\n",
    "\n",
    "df[\"run\"] = df[\"run\"].apply(lambda x: \"run_\"+str(x))    \n",
    "\n",
    "\n",
    "#sizez proportianal to max amplitude of that speaket \n",
    "\n",
    "df[\"size\"] = sizes\n",
    "\n",
    "\n",
    "\n",
    "fig = px.scatter_3d(df, x=\"x\", y=\"y\", z=\"z\", color=\"speaker\",size=\"size\",hover_data=[\"run\"])\n",
    "\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = px.scatter_3d(df, x=\"x\", y=\"y\", z=\"z\", color=\"run\")\n",
    "fig.show()\n",
    "\n",
    "print(\"vidi se posamezne speakerje in clustre ki so mixi vecih speakerjev naenkrat ko ne locimo dobro\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot all centroids on umap only mic x\n",
    "\n",
    "mic_to_plot = 1\n",
    "\n",
    "centroid_speaker_labels_run = []\n",
    "\n",
    "runsss= []\n",
    "\n",
    "sizes  = []\n",
    "\n",
    "current_centroids = all_centroids[mic_to_plot]\n",
    "\n",
    "for j in range(current_centroids.shape[0]):\n",
    "    runsss.append(mic_to_plot)\n",
    "    centroid_speaker_labels_run.append(f\"mic_{mic_to_plot}_speaker_{j}\")\n",
    "    speaker_key = f\"SPEAKER_{j:02d}\"\n",
    "    sizes.append(all_max_amplitude[mic_to_plot].get(speaker_key, None))  # Default size to 1 if key not found\n",
    "\n",
    "print(centroid_speaker_labels_run)\n",
    "\n",
    "all_centroids_flat = current_centroids\n",
    "\n",
    "print(all_centroids_flat.shape)\n",
    "\n",
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(metric=\"cosine\",n_components=3)\n",
    "\n",
    "umap_centroids = reducer.fit_transform(all_centroids_flat)\n",
    "\n",
    "#plotly\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(umap_centroids, columns=[\"x\", \"y\",\"z\"])\n",
    "\n",
    "df[\"speaker\"] = centroid_speaker_labels_run\n",
    "\n",
    "#to string\n",
    "\n",
    "df[\"run\"] = runsss\n",
    "\n",
    "df[\"run\"] = df[\"run\"].apply(lambda x: \"run_\"+str(x))\n",
    "\n",
    "#sizez proportianal to max amplitude of that speaket\n",
    "\n",
    "df[\"size\"] = sizes\n",
    "\n",
    "fig = px.scatter_3d(df, x=\"x\", y=\"y\", z=\"z\", color=\"speaker\",size=\"size\",hover_data=[\"run\"])\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Collect all unique labels for the y-axis\n",
    "unique_labels = []\n",
    "\n",
    "for i in range(len(all_diars)):\n",
    "    current_diar = all_diars[i]\n",
    "    for speaker in current_diar.labels():\n",
    "        unique_label = f\"mic_{i}_{speaker}\"\n",
    "        unique_labels.append(unique_label)\n",
    "\n",
    "# Remove duplicates and sort labels\n",
    "unique_labels = sorted(set(unique_labels))\n",
    "\n",
    "for i in range(len(all_diars)):\n",
    "    current_diar = all_diars[i]\n",
    "\n",
    "    #get representitive speakers for each run\n",
    "    current_max_amplitude = all_max_amplitude[i]\n",
    "    \n",
    "\n",
    "\n",
    "    run_color = px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)]\n",
    "\n",
    "    for turn, _, speaker in current_diar.itertracks(yield_label=True):\n",
    "        y_label = f\"mic_{i}_{speaker}\"\n",
    "\n",
    "        #this is flat signal with stacked source separatin channels \n",
    "\n",
    "        # print(turn.start, turn.start - (300 * (turn.start // 300)))\n",
    "        start = np.rint(turn.start % 300)\n",
    "        end = np.rint(turn.end % 300)\n",
    "\n",
    "        # Handle special cases\n",
    "        if end < start and np.allclose(start, 300):\n",
    "            start = 0\n",
    "\n",
    "        if start > end and np.allclose(end, 0):\n",
    "            end = 300\n",
    "\n",
    "            \n",
    "        # if y_label == \"mic_2_SPEAKER_03\":\n",
    "        #     print(start,end,turn.start,turn.end)\n",
    "\n",
    "\n",
    "        #calculate max amplitude of this segment\n",
    "\n",
    "        segment = pipeline._audio.crop(segment=turn,file=audio_in_memory)\n",
    "        segment=segment[0]\n",
    "\n",
    "        segment = segment.squeeze(0).numpy()\n",
    "\n",
    "        def calculate_energy(signal):\n",
    "            return np.sum(signal ** 2)\n",
    "\n",
    "        energy = calculate_energy(segment)\n",
    "\n",
    "        #energy values are rea\n",
    "\n",
    "        #use plotly colors scale to get color if representitive speaker\n",
    "\n",
    "        #check if the speaker is the representitive speaker\n",
    "        #fin the index if the curent speaker in current_max_amplitude\n",
    "        speaker_index = list(current_max_amplitude.keys()).index(speaker)\n",
    "        # print(speaker_index)\n",
    "\n",
    "        #get max amplitude of the speaker\n",
    "\n",
    "        #get color based on index  use plotly colors make the first one really hot\n",
    "        c = px.colors.sequential.Reds[(8-speaker_index)]\n",
    "\n",
    "        #max_amplitude\n",
    "        # print(max_amplitude)\n",
    "\n",
    "        \n",
    "\n",
    "        # cc = int(energy*100000)\n",
    "        # #max at 100\n",
    "        # cc = min(cc,5)\n",
    "        # print(energy)\n",
    "        # c = px.colors.sequential.Reds[cc]\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=[start,end],\n",
    "            y=[y_label, y_label],\n",
    "            mode='lines',\n",
    "            line=dict(color=c, width=10),\n",
    "            name=y_label,\n",
    "            legendgroup=f\"mic_{i}\",\n",
    "            showlegend=(y_label not in [trace.name for trace in fig.data])\n",
    "        ))\n",
    "\n",
    "# Update layout to set y-axis as category type and use the unique labels\n",
    "fig.update_layout(\n",
    "    yaxis=dict(\n",
    "        title='Speakers',\n",
    "        tickmode='array',\n",
    "        tickvals=unique_labels,\n",
    "        ticktext=unique_labels,\n",
    "        categoryorder='array',\n",
    "        categoryarray=unique_labels\n",
    "    ),\n",
    "    xaxis=dict(title='Time'),\n",
    "    title='Diarization Visualization',\n",
    "    legend=dict(title='Speakers', itemsizing='constant')\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Collect all unique labels for the y-axis\n",
    "unique_labels = []\n",
    "\n",
    "\n",
    "i = 0\n",
    "\n",
    "\n",
    "current_diar = all_diars[i]\n",
    "for speaker in current_diar.labels():\n",
    "    unique_label = f\"mic_{i}_{speaker}\"\n",
    "    unique_labels.append(unique_label)\n",
    "\n",
    "# Remove duplicates and sort labels\n",
    "unique_labels.append(\"GT\")\n",
    "unique_labels = sorted(set(unique_labels))\n",
    "\n",
    "#add gt label\n",
    "gt_Segments= [[66,70],[75,77],[113,118],[155,171],[171.5,172],[225,239],[249,254]]\n",
    "\n",
    "for start,end in gt_Segments:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[start,end],\n",
    "        y=[\"GT\", \"GT\"],\n",
    "        mode='lines',\n",
    "        line=dict(color=\"green\", width=10),\n",
    "        name=y_label,\n",
    "        legendgroup=f\"mic_{i}\",\n",
    "        showlegend=(y_label not in [trace.name for trace in fig.data])\n",
    "    ))\n",
    "\n",
    "current_diar = all_diars[i]\n",
    "\n",
    "#get representitive speakers for each run\n",
    "current_max_amplitude = all_max_amplitude[i]\n",
    "\n",
    "\n",
    "\n",
    "run_color = px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)]\n",
    "\n",
    "for turn, _, speaker in current_diar.itertracks(yield_label=True):\n",
    "    y_label = f\"mic_{i}_{speaker}\"\n",
    "\n",
    "    #this is flat signal with stacked source separatin channels \n",
    "\n",
    "    # print(turn.start, turn.start - (300 * (turn.start // 300)))\n",
    "    start = np.rint(turn.start % 300)\n",
    "    end = np.rint(turn.end % 300)\n",
    "\n",
    "    # Handle special cases\n",
    "    if end < start and np.allclose(start, 300):\n",
    "        start = 0\n",
    "\n",
    "    if start > end and np.allclose(end, 0):\n",
    "        end = 300\n",
    "\n",
    "        \n",
    "    # if y_label == \"mic_2_SPEAKER_03\":\n",
    "    #     print(start,end,turn.start,turn.end)\n",
    "\n",
    "\n",
    "    #calculate max amplitude of this segment\n",
    "\n",
    "    segment = pipeline._audio.crop(segment=turn,file=audio_in_memory)\n",
    "    segment=segment[0]\n",
    "\n",
    "    segment = segment.squeeze(0).numpy()\n",
    "\n",
    "    def calculate_energy(signal):\n",
    "        return np.sum(signal ** 2)\n",
    "\n",
    "    energy = calculate_energy(segment)\n",
    "\n",
    "    #energy values are rea\n",
    "\n",
    "    #use plotly colors scale to get color if representitive speaker\n",
    "\n",
    "    #check if the speaker is the representitive speaker\n",
    "    #fin the index if the curent speaker in current_max_amplitude\n",
    "    speaker_index = list(current_max_amplitude.keys()).index(speaker)\n",
    "    # print(speaker_index)\n",
    "\n",
    "    #get max amplitude of the speaker\n",
    "\n",
    "    #get color based on index  use plotly colors make the first one really hot\n",
    "    c = px.colors.sequential.Reds[(8-speaker_index)]\n",
    "\n",
    "    #max_amplitude\n",
    "    # print(max_amplitude)\n",
    "\n",
    "    \n",
    "\n",
    "    cc = int(energy*100000)\n",
    "    #max at 100\n",
    "    cc = min(cc,5)\n",
    "    print(energy)\n",
    "    c = px.colors.sequential.Reds[cc]\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[start,end],\n",
    "        y=[y_label, y_label],\n",
    "        mode='lines',\n",
    "        line=dict(color=c, width=10),\n",
    "        name=y_label,\n",
    "        legendgroup=f\"mic_{i}\",\n",
    "        showlegend=(y_label not in [trace.name for trace in fig.data])\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "# Update layout to set y-axis as category type and use the unique labels\n",
    "fig.update_layout(\n",
    "yaxis=dict(\n",
    "    title='Speakers',\n",
    "    tickmode='array',\n",
    "    tickvals=unique_labels,\n",
    "    ticktext=unique_labels,\n",
    "    categoryorder='array',\n",
    "    categoryarray=unique_labels\n",
    "),\n",
    "xaxis=dict(title='Time'),\n",
    "title='Diarization Visualization',\n",
    "legend=dict(title='Speakers', itemsizing='constant')\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Collect all unique labels for the y-axis\n",
    "unique_labels = []\n",
    "\n",
    "\n",
    "i = 1\n",
    "\n",
    "\n",
    "current_diar = all_diars[i]\n",
    "for speaker in current_diar.labels():\n",
    "    unique_label = f\"mic_{i}_{speaker}\"\n",
    "    unique_labels.append(unique_label)\n",
    "\n",
    "# Remove duplicates and sort labels\n",
    "unique_labels.append(\"GT\")\n",
    "unique_labels = sorted(set(unique_labels))\n",
    "\n",
    "#add gt label\n",
    "gt_Segments= [[0,9],[15,17.9],[18.2,20.4],[29.8,30.10],[39.5,39.8],[40.8,49.37],[52.10,52.5],[60.13,60.5],[63.30,64.3],[201.8,207.8],[212.4,212.6],[215.1,216.1],[223.3,225],[247.7,249.8]]\n",
    "for start,end in gt_Segments:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[start,end],\n",
    "        y=[\"GT\", \"GT\"],\n",
    "        mode='lines',\n",
    "        line=dict(color=\"green\", width=10),\n",
    "        name=y_label,\n",
    "        legendgroup=f\"mic_{i}\",\n",
    "        showlegend=(y_label not in [trace.name for trace in fig.data])\n",
    "    ))\n",
    "\n",
    "current_diar = all_diars[i]\n",
    "\n",
    "#get representitive speakers for each run\n",
    "current_max_amplitude = all_max_amplitude[i]\n",
    "\n",
    "\n",
    "\n",
    "run_color = px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)]\n",
    "\n",
    "for turn, _, speaker in current_diar.itertracks(yield_label=True):\n",
    "    y_label = f\"mic_{i}_{speaker}\"\n",
    "\n",
    "    #this is flat signal with stacked source separatin channels \n",
    "\n",
    "    # print(turn.start, turn.start - (300 * (turn.start // 300)))\n",
    "    start = np.rint(turn.start % 300)\n",
    "    end = np.rint(turn.end % 300)\n",
    "\n",
    "    # Handle special cases\n",
    "    if end < start and np.allclose(start, 300):\n",
    "        start = 0\n",
    "\n",
    "    if start > end and np.allclose(end, 0):\n",
    "        end = 300\n",
    "\n",
    "        \n",
    "    # if y_label == \"mic_2_SPEAKER_03\":\n",
    "    #     print(start,end,turn.start,turn.end)\n",
    "\n",
    "\n",
    "    #calculate max amplitude of this segment\n",
    "\n",
    "    segment = pipeline._audio.crop(segment=turn,file=audio_in_memory)\n",
    "    segment=segment[0]\n",
    "\n",
    "    segment = segment.squeeze(0).numpy()\n",
    "\n",
    "    def calculate_energy(signal):\n",
    "        return np.sum(signal ** 2)\n",
    "\n",
    "    energy = calculate_energy(segment)\n",
    "\n",
    "    #energy values are rea\n",
    "\n",
    "    #use plotly colors scale to get color if representitive speaker\n",
    "\n",
    "    #check if the speaker is the representitive speaker\n",
    "    #fin the index if the curent speaker in current_max_amplitude\n",
    "    speaker_index = list(current_max_amplitude.keys()).index(speaker)\n",
    "    # print(speaker_index)\n",
    "\n",
    "    #get max amplitude of the speaker\n",
    "\n",
    "    #get color based on index  use plotly colors make the first one really hot\n",
    "    c = px.colors.sequential.Reds[(8-speaker_index)]\n",
    "\n",
    "    #max_amplitude\n",
    "    # print(max_amplitude)\n",
    "\n",
    "    \n",
    "\n",
    "    # cc = int(energy*100000)\n",
    "    # #max at 100\n",
    "    # cc = min(cc,5)\n",
    "    # print(energy)\n",
    "    # c = px.colors.sequential.Reds[cc]\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[start,end],\n",
    "        y=[y_label, y_label],\n",
    "        mode='lines',\n",
    "        line=dict(color=c, width=10),\n",
    "        name=y_label,\n",
    "        legendgroup=f\"mic_{i}\",\n",
    "        showlegend=(y_label not in [trace.name for trace in fig.data])\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "# Update layout to set y-axis as category type and use the unique labels\n",
    "fig.update_layout(\n",
    "yaxis=dict(\n",
    "    title='Speakers',\n",
    "    tickmode='array',\n",
    "    tickvals=unique_labels,\n",
    "    ticktext=unique_labels,\n",
    "    categoryorder='array',\n",
    "    categoryarray=unique_labels\n",
    "),\n",
    "xaxis=dict(title='Time'),\n",
    "title='Diarization Visualization',\n",
    "legend=dict(title='Speakers', itemsizing='constant')\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Collect all unique labels for the y-axis\n",
    "unique_labels = []\n",
    "\n",
    "\n",
    "i = 5\n",
    "\n",
    "\n",
    "current_diar = all_diars[i]\n",
    "for speaker in current_diar.labels():\n",
    "    unique_label = f\"mic_{i}_{speaker}\"\n",
    "    unique_labels.append(unique_label)\n",
    "\n",
    "# Remove duplicates and sort labels\n",
    "unique_labels.append(\"GT\")\n",
    "unique_labels = sorted(set(unique_labels))\n",
    "\n",
    "\n",
    "\n",
    "#add gt label\n",
    "gt_Segments= [[10.7,14.4],[21.7,41.5],[48.4,63.2],[95.8,96.8],[102.2,102.8],[109.6,111.0],[114,118],[128,144],[155,155.8],[183,202.2],[261.3,262],[268.9,269],[274.2,278.7],[281.8,282]]\n",
    "for start,end in gt_Segments:\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[start,end],\n",
    "        y=[\"GT\", \"GT\"],\n",
    "        mode='lines',\n",
    "        line=dict(color=\"green\", width=10),\n",
    "        name=y_label,\n",
    "        legendgroup=f\"mic_{i}\",\n",
    "        showlegend=(y_label not in [trace.name for trace in fig.data])\n",
    "    ))\n",
    "\n",
    "current_diar = all_diars[i]\n",
    "\n",
    "#get representitive speakers for each run\n",
    "current_max_amplitude = all_max_amplitude[i]\n",
    "\n",
    "\n",
    "\n",
    "run_color = px.colors.qualitative.Plotly[i % len(px.colors.qualitative.Plotly)]\n",
    "\n",
    "for turn, _, speaker in current_diar.itertracks(yield_label=True):\n",
    "    y_label = f\"mic_{i}_{speaker}\"\n",
    "\n",
    "    #this is flat signal with stacked source separatin channels \n",
    "\n",
    "    # print(turn.start, turn.start - (300 * (turn.start // 300)))\n",
    "    start = np.rint(turn.start % 300)\n",
    "    end = np.rint(turn.end % 300)\n",
    "\n",
    "    # Handle special cases\n",
    "    if end < start and np.allclose(start, 300):\n",
    "        start = 0\n",
    "\n",
    "    if start > end and np.allclose(end, 0):\n",
    "        end = 300\n",
    "\n",
    "        \n",
    "    # if y_label == \"mic_2_SPEAKER_03\":\n",
    "    #     print(start,end,turn.start,turn.end)\n",
    "\n",
    "\n",
    "    #calculate max amplitude of this segment\n",
    "\n",
    "    segment = pipeline._audio.crop(segment=turn,file=audio_in_memory)\n",
    "    segment=segment[0]\n",
    "\n",
    "    segment = segment.squeeze(0).numpy()\n",
    "\n",
    "    def calculate_energy(signal):\n",
    "        return np.sum(signal ** 2)\n",
    "\n",
    "    energy = calculate_energy(segment)\n",
    "\n",
    "    #energy values are rea\n",
    "\n",
    "    #use plotly colors scale to get color if representitive speaker\n",
    "\n",
    "    #check if the speaker is the representitive speaker\n",
    "    #fin the index if the curent speaker in current_max_amplitude\n",
    "    speaker_index = list(current_max_amplitude.keys()).index(speaker)\n",
    "    # print(speaker_index)\n",
    "\n",
    "    #get max amplitude of the speaker\n",
    "\n",
    "    #get color based on index  use plotly colors make the first one really hot\n",
    "    c = px.colors.sequential.Reds[(8-speaker_index)]\n",
    "\n",
    "    #max_amplitude\n",
    "    # print(max_amplitude)\n",
    "\n",
    "    \n",
    "\n",
    "    # cc = int(energy*100000)\n",
    "    # #max at 100\n",
    "    # cc = min(cc,5)\n",
    "    # print(energy)\n",
    "    # c = px.colors.sequential.Reds[cc]\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[start,end],\n",
    "        y=[y_label, y_label],\n",
    "        mode='lines',\n",
    "        line=dict(color=c, width=10),\n",
    "        name=y_label,\n",
    "        legendgroup=f\"mic_{i}\",\n",
    "        showlegend=(y_label not in [trace.name for trace in fig.data])\n",
    "    ))\n",
    "\n",
    "\n",
    "\n",
    "# Update layout to set y-axis as category type and use the unique labels\n",
    "fig.update_layout(\n",
    "yaxis=dict(\n",
    "    title='Speakers',\n",
    "    tickmode='array',\n",
    "    tickvals=unique_labels,\n",
    "    ticktext=unique_labels,\n",
    "    categoryorder='array',\n",
    "    categoryarray=unique_labels\n",
    "),\n",
    "xaxis=dict(title='Time'),\n",
    "title='Diarization Visualization',\n",
    "legend=dict(title='Speakers', itemsizing='constant')\n",
    ")\n",
    "\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exatract audio of mic x  speaker x\n",
    "\n",
    "mic_index =1\n",
    "speaker_indexes= [5,1]\n",
    "speaker_indexes = [f\"SPEAKER_0{i}\" for i in speaker_indexes]\n",
    "\n",
    "\n",
    "#get audio singal at chunks of speaker 1\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "mic = data[mic_index]\n",
    "\n",
    "\n",
    "sr = 16000\n",
    "\n",
    "print(mic.shape)\n",
    "\n",
    "#(30, 6, 160000) -> (6, 160000 * 30)\n",
    "\n",
    "mic = einops.rearrange(mic, \"a b c -> b (a c)\")\n",
    "\n",
    "print(mic.shape) \n",
    "\n",
    "\n",
    "#(6, 4800000) -> (14800000 * 6)\n",
    "\n",
    "mic_full_flat = einops.rearrange(mic, \"a b -> (a b)\")\n",
    "\n",
    "speaker1 = np.zeros_like(mic_full_flat)\n",
    "\n",
    "print(mic_full_flat.shape) #(28800000,)\n",
    "\n",
    "for turn, _, speaker in all_diars[mic_index].itertracks(yield_label=True):\n",
    "    if speaker in speaker_indexes:\n",
    "        start = int(turn.start * sr)\n",
    "        end = int(turn.end * sr)\n",
    "        speaker1[start:end] = mic_full_flat[start:end]\n",
    "\n",
    "print(speaker1.shape)\n",
    "#(28800000,) -> 4800000, 6)\n",
    "\n",
    "#\n",
    "\n",
    "#reshape it by sequentaly making chunks of 4800000\n",
    "\n",
    "res = np.zeros((6,speaker1.shape[0]//6))\n",
    "for i in range(6):\n",
    "    res[i]=speaker1[i*speaker1.shape[0]//6:(i+1)*speaker1.shape[0]//6]\n",
    "\n",
    "print(res.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sf.write(\"outputttt3_mic55_flaten_speaker_x.wav\", res.T, sr)\n",
    "\n",
    "#TODO NO SAVING NEEDED\n",
    "# audio_in_memory = {\"waveform\": waveform, \"sample_rate\": sample_rate}\n",
    "# type(waveform)=<class 'torch.Tensor'>\n",
    "# waveform.shape=torch.Size([1, 480000])\n",
    "# waveform.dtype=torch.float32\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
